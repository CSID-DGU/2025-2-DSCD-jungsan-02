import re
import threading
import fcntl
import os
from functools import lru_cache
from typing import Optional, List

import torch
from soynlp.normalizer import emoticon_normalize
from transformers import T5ForConditionalGeneration, T5Tokenizer


MODEL_NAME = "j5ng/et5-typos-corrector"

# ê²€ìƒ‰ ìµœì í™”: ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê²€ìƒ‰ì— ë„ì›€ì´ ì•ˆ ë˜ëŠ” ë‹¨ì–´)
# ì¡°ì‚¬, ì–´ë¯¸, ë¶„ì‹¤ ê´€ë ¨ ë™ì‚¬ ë“± ê²€ìƒ‰ì— ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
STOPWORDS = {
    # ì¡°ì‚¬
    'ì—ì„œ', 'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ',
    'ì€', 'ëŠ”', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'ê»˜', 'í•œí…Œ', 'ì²˜ëŸ¼', 'ê°™ì´',
    # ì–´ë¯¸/ì¢…ê²°ì–´ë¯¸
    'ì…ë‹ˆë‹¤', 'ì´ì—ìš”', 'ì˜ˆìš”', 'ì´ì•¼', 'ì•¼', 'ë‹¤', 'ì–´', 'ì•„', 'ë„¤', 'ì§€',
    # ë¶„ì‹¤ ê´€ë ¨ ë™ì‚¬ (ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì•„ë‹˜)
    'ìƒì–´ë²„ë¦°', 'ìƒì–´ë²„ë¦°', 'ë¶„ì‹¤í•œ', 'ìƒì€', 'ì°¾ëŠ”', 'ì°¾ê³ ìˆëŠ”', 'ì°¾ê³ ', 'ì°¾ì•„',
    'ë°œê²¬í•œ', 'ë°œê²¬', 'ìŠµë“í•œ', 'ìŠµë“', 'ì£¼ìš´', 'ì£¼ì› ', 'ì°¾ì•„ì£¼ì„¸ìš”', 'ì°¾ì•„ìš”',
    # ì¼ë°˜ ë™ì‚¬/í˜•ìš©ì‚¬ (ê²€ìƒ‰ì— ë¶ˆí•„ìš”)
    'ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', 'ìˆì–´', 'ì—†ì–´ìš”', 'ì—†ìŠµë‹ˆë‹¤', 'ì—†ì–´',
    'ê°™ì•„ìš”', 'ê°™ìŠµë‹ˆë‹¤', 'ê°™ì•„', 'ë‹¤ë¥¸', 'ë‹¤ë¥´',
    # ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ë‹¨ì–´
    'ì œ', 'ë‚´', 'ì €ì˜', 'ë‚˜ì˜', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ê·¸ê²ƒ', 'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°',
    'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ë–¤', 'ì–´ë–»ê²Œ',
}

# ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥ ì‚¬ì „ (ë™ì˜ì–´/ìœ ì‚¬ì–´)
# ë¶„ì‹¤ë¬¼ ê²€ìƒ‰ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œë“¤ì˜ ë™ì˜ì–´ í™•ì¥
KEYWORD_EXPANSION = {
    # ì‹ ë°œë¥˜
    'ìš´ë™í™”': ['ìš´ë™í™”', 'ì‹ ë°œ', 'ìŠ¤ë‹ˆì»¤ì¦ˆ', 'ìŠ¤ë‹ˆì»¤', 'ìš´ë™í™”', 'êµ¬ë‘', 'ì‹ ë°œ'],
    'êµ¬ë‘': ['êµ¬ë‘', 'ì‹ ë°œ', 'ìš´ë™í™”', 'í•˜ì´í', 'ë¡œí¼'],
    'ë¶€ì¸ ': ['ë¶€ì¸ ', 'ì¥í™”', 'ë¶€í‹°', 'ì›Œì»¤'],
    'ìƒŒë“¤': ['ìƒŒë“¤', 'ìŠ¬ë¦¬í¼', 'í”Œë¦½í”Œë¡­'],
    
    # ì§€ê°‘ë¥˜
    'ì§€ê°‘': ['ì§€ê°‘', 'ë°˜ì§€ê°‘', 'ì¥ì§€ê°‘', 'ì¹´ë“œì§€ê°‘', 'ëª…í•¨ì§€ê°‘', 'ì½”ì¸ì§€ê°‘'],
    'ë°˜ì§€ê°‘': ['ë°˜ì§€ê°‘', 'ì§€ê°‘', 'ì¹´ë“œì§€ê°‘'],
    'ì¥ì§€ê°‘': ['ì¥ì§€ê°‘', 'ì§€ê°‘', 'ëª…í•¨ì§€ê°‘'],
    
    # ê°€ë°©ë¥˜
    'ê°€ë°©': ['ê°€ë°©', 'ë°±íŒ©', 'í¬ë¡œìŠ¤ë°±', 'í† íŠ¸ë°±', 'ìˆ„ë”ë°±', 'ë°±', 'ì„œë¥˜ê°€ë°©'],
    'ë°±íŒ©': ['ë°±íŒ©', 'ë°°ë‚­', 'ê°€ë°©', 'ì±…ê°€ë°©'],
    'í¬ë¡œìŠ¤ë°±': ['í¬ë¡œìŠ¤ë°±', 'ê°€ë°©', 'ë©”ì‹ ì €ë°±'],
    'í† íŠ¸ë°±': ['í† íŠ¸ë°±', 'ê°€ë°©', 'ì‡¼í•‘ë°±'],
    'í•¸ë“œë°±': ['í•¸ë“œë°±', 'ê°€ë°©', 'í´ëŸ¬ì¹˜ë°±'],
    
    # ì „ìê¸°ê¸°
    'í•¸ë“œí°': ['í•¸ë“œí°', 'ìŠ¤ë§ˆíŠ¸í°', 'íœ´ëŒ€í°', 'ì•„ì´í°', 'ê°¤ëŸ­ì‹œ', 'í°', 'ì „í™”ê¸°'],
    'ìŠ¤ë§ˆíŠ¸í°': ['ìŠ¤ë§ˆíŠ¸í°', 'í•¸ë“œí°', 'íœ´ëŒ€í°', 'í°'],
    'ì•„ì´í°': ['ì•„ì´í°', 'ìŠ¤ë§ˆíŠ¸í°', 'í•¸ë“œí°', 'í°'],
    'ê°¤ëŸ­ì‹œ': ['ê°¤ëŸ­ì‹œ', 'ìŠ¤ë§ˆíŠ¸í°', 'í•¸ë“œí°', 'í°'],
    'ë…¸íŠ¸ë¶': ['ë…¸íŠ¸ë¶', 'ë©í†±', 'ì»´í“¨í„°', 'ë§¥ë¶', 'ê·¸ë¨'],
    'íƒœë¸”ë¦¿': ['íƒœë¸”ë¦¿', 'ì•„ì´íŒ¨ë“œ', 'ê°¤ëŸ­ì‹œíƒ­', 'íŒ¨ë“œ'],
    'ì´ì–´í°': ['ì´ì–´í°', 'í—¤ë“œí°', 'ì—ì–´íŒŸ', 'ë²„ì¦ˆ', 'ì´ì–´ë²„ë“œ'],
    'ì¶©ì „ê¸°': ['ì¶©ì „ê¸°', 'ì¼€ì´ë¸”', 'ì–´ëŒ‘í„°', 'ì „ì›ì„ '],
    
    # ì•¡ì„¸ì„œë¦¬
    'ì‹œê³„': ['ì‹œê³„', 'ì†ëª©ì‹œê³„', 'ìŠ¤ë§ˆíŠ¸ì›Œì¹˜', 'ì›Œì¹˜'],
    'ì•ˆê²½': ['ì•ˆê²½', 'ì„ ê¸€ë¼ìŠ¤', 'ë Œì¦ˆ', 'ë‹ë³´ê¸°'],
    'ëª©ê±¸ì´': ['ëª©ê±¸ì´', 'íœë˜íŠ¸', 'ì²´ì¸'],
    'ë°˜ì§€': ['ë°˜ì§€', 'ë§'],
    'ê·€ê±¸ì´': ['ê·€ê±¸ì´', 'ì´ì–´ë§'],
    
    # ê¸°íƒ€ ì¼ìƒìš©í’ˆ
    'ì—´ì‡ ': ['ì—´ì‡ ', 'í‚¤', 'í‚¤ë§'],
    'ìš°ì‚°': ['ìš°ì‚°', 'ì–‘ì‚°', 'ì¥ìš°ì‚°', 'ì ‘ì´ì‹ìš°ì‚°'],
    'ì§€ê°‘': ['ì§€ê°‘', 'ë°˜ì§€ê°‘', 'ì¥ì§€ê°‘'],
    'ë§ˆìŠ¤í¬': ['ë§ˆìŠ¤í¬', 'ë§ˆìŠ¤í¬', 'KF94', 'KF80'],
    'ì¥ê°‘': ['ì¥ê°‘', 'ê¸€ëŸ¬ë¸Œ', 'ì†ëª©ì¥ê°‘'],
    'ëª¨ì': ['ëª¨ì', 'ìº¡', 'ì•¼êµ¬ëª¨ì', 'ë¹„ë‹ˆ'],
    'ìŠ¤ì¹´í”„': ['ìŠ¤ì¹´í”„', 'ëª©ë„ë¦¬', 'ë¨¸í”ŒëŸ¬'],
    
    # ì˜ë¥˜
    'ì˜·': ['ì˜·', 'ì˜ë¥˜', 'ìƒì˜', 'í•˜ì˜'],
    'ì…”ì¸ ': ['ì…”ì¸ ', 'ì™€ì´ì…”ì¸ ', 'ë“œë ˆìŠ¤ì…”ì¸ ', 'ë¸”ë¼ìš°ìŠ¤', 'ìƒì˜', 'ì˜·'],
    'ì²´í¬ì…”ì¸ ': ['ì²´í¬ì…”ì¸ ', 'ì²´í¬ ì…”ì¸ ', 'ì²´í¬ì™€ì´ì…”ì¸ ', 'ì²´í¬', 'ì…”ì¸ '],
    'ì²´í¬': ['ì²´í¬', 'ì²´í¬ë¬´ëŠ¬', 'ì²´í¬íŒ¨í„´', 'ì²´í¬ì…”ì¸ ', 'ì²´í¬ì™€ì´ì…”ì¸ '],
    'ìŠ¤íŠ¸ë¼ì´í”„': ['ìŠ¤íŠ¸ë¼ì´í”„', 'ì¤„ë¬´ëŠ¬', 'ì¤„', 'ìŠ¤íŠ¸ë¼ì´í”„ì…”ì¸ ', 'ìŠ¤íŠ¸ë¼ì´í”„í‹°ì…”ì¸ '],
    'ì¤„ë¬´ëŠ¬': ['ì¤„ë¬´ëŠ¬', 'ìŠ¤íŠ¸ë¼ì´í”„', 'ì¤„', 'ì„¸ë¡œì¤„', 'ê°€ë¡œì¤„'],
    'í‹°ì…”ì¸ ': ['í‹°ì…”ì¸ ', 'í‹°', 'ë°˜íŒ”', 'ê¸´íŒ”', 'ìƒì˜'],
    'ì¬í‚·': ['ì¬í‚·', 'ìì¼“', 'ì•„ìš°í„°'],
    'ì½”íŠ¸': ['ì½”íŠ¸', 'ì•„ìš°í„°', 'ì™¸íˆ¬'],
    'í›„ë“œ': ['í›„ë“œ', 'í›„ë“œí‹°', 'í›„ë“œì§‘ì—…'],
    'ë°”ì§€': ['ë°”ì§€', 'íŒ¬ì¸ ', 'ì²­ë°”ì§€', 'ìŠ¬ë™ìŠ¤', 'í•˜ì˜'],
    'ì²­ë°”ì§€': ['ì²­ë°”ì§€', 'ë°ë‹˜', 'ë°”ì§€', 'í•˜ì˜'],
    
    # ìƒ‰ìƒ (ì¼ë¶€ ì£¼ìš” ìƒ‰ìƒ)
    'ê²€ì€ìƒ‰': ['ê²€ì€ìƒ‰', 'ê²€ì •', 'ê²€ì •ìƒ‰', 'ë¸”ë™'],
    'í°ìƒ‰': ['í°ìƒ‰', 'í•˜ì–‘', 'í•˜ì–€ìƒ‰', 'í™”ì´íŠ¸'],
    'ë¹¨ê°„ìƒ‰': ['ë¹¨ê°„ìƒ‰', 'ë¹¨ê°•', 'ë¹¨ê°•ìƒ‰', 'ë ˆë“œ'],
    'íŒŒë€ìƒ‰': ['íŒŒë€ìƒ‰', 'íŒŒë‘', 'íŒŒë‘ìƒ‰', 'ë¸”ë£¨'],
    'ë…¸ë€ìƒ‰': ['ë…¸ë€ìƒ‰', 'ë…¸ë‘', 'ë…¸ë‘ìƒ‰', 'ì˜ë¡œìš°'],
    'ì´ˆë¡ìƒ‰': ['ì´ˆë¡ìƒ‰', 'ì´ˆë¡', 'ë…¹ìƒ‰', 'ê·¸ë¦°'],
    
    # ë¸Œëœë“œ (ì£¼ìš” ë¸Œëœë“œ)
    'ë‚˜ì´í‚¤': ['ë‚˜ì´í‚¤', 'ë‹ˆì¼€', 'nike'],
    'ì•„ë””ë‹¤ìŠ¤': ['ì•„ë””ë‹¤ìŠ¤', 'ì•„ë””', 'adidas'],
    'ìƒ˜ì†Œë‚˜ì´íŠ¸': ['ìƒ˜ì†Œë‚˜ì´íŠ¸', 'samsonite'],
    'êµ¬ì°Œ': ['êµ¬ì°Œ', 'gucci'],
    'í”„ë¼ë‹¤': ['í”„ë¼ë‹¤', 'prada'],
    'ë£¨ì´ë¹„í†µ': ['ë£¨ì´ë¹„í†µ', 'lv', 'louis vuitton'],
}


# ì „ì—­ ë½ ë° ëª¨ë¸ ìºì‹œ (ì›Œì»¤ ê°„ ê³µìœ ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜)
_tokenizer_lock = threading.Lock()
_model_lock = threading.Lock()
_tokenizer_cache = None
_model_cache = None

def _load_tokenizer() -> T5Tokenizer:
    """í† í¬ë‚˜ì´ì € ë¡œë“œ (íŒŒì¼ ë½ìœ¼ë¡œ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€)"""
    global _tokenizer_cache
    
    if _tokenizer_cache is not None:
        return _tokenizer_cache
    
    with _tokenizer_lock:
        # ì´ì¤‘ ì²´í¬
        if _tokenizer_cache is not None:
            return _tokenizer_cache
        
        _tokenizer_cache = T5Tokenizer.from_pretrained(MODEL_NAME)
        return _tokenizer_cache


def _load_model() -> T5ForConditionalGeneration:
    """ëª¨ë¸ ë¡œë“œ (íŒŒì¼ ë½ìœ¼ë¡œ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë° GPU ë©”ëª¨ë¦¬ ì¶©ëŒ ë°©ì§€)"""
    global _model_cache
    
    if _model_cache is not None:
        return _model_cache
    
    # íŒŒì¼ ë½ ê²½ë¡œ (ì›Œì»¤ ê°„ ê³µìœ )
    lock_file_path = os.path.join("/tmp", ".typo_model_load_lock")
    os.makedirs(os.path.dirname(lock_file_path), exist_ok=True)
    
    with _model_lock:
        # ì´ì¤‘ ì²´í¬
        if _model_cache is not None:
            return _model_cache
        
        # íŒŒì¼ ë½ìœ¼ë¡œ ë‹¤ë¥¸ ì›Œì»¤ì™€ì˜ ì¶©ëŒ ë°©ì§€
        with open(lock_file_path, 'w') as lock_file:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
            
            try:
                # ë‹¤ì‹œ í™•ì¸ (íŒŒì¼ ë½ íšë“ í›„)
                if _model_cache is not None:
                    return _model_cache
                
                print(f"ğŸ“¥ ë§ì¶¤ë²• êµì • ëª¨ë¸ ë¡œë“œ ì‹œì‘: {MODEL_NAME}")
                model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
                
                # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ë©”ëª¨ë¦¬ ì²´í¬
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    allocated = torch.cuda.memory_allocated(0) / (1024**3)
                    free = gpu_memory - allocated
                    
                    # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´ GPU ì‚¬ìš©, ì•„ë‹ˆë©´ CPU ì‚¬ìš©
                    if free > 1.0:  # ìµœì†Œ 1GB í•„ìš”
                        device = torch.device("cuda")
                        print(f"ğŸ’¾ GPU ì‚¬ìš©: ì—¬ìœ  {free:.2f}GB")
                    else:
                        device = torch.device("cpu")
                        print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ({free:.2f}GB), CPU ì‚¬ìš©")
                else:
                    device = torch.device("cpu")
                    print("ğŸ’¾ CPU ì‚¬ìš© (GPU ì—†ìŒ)")
                
                # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (to_empty ì‚¬ìš©í•˜ì—¬ meta tensor ë¬¸ì œ í•´ê²°)
                try:
                    model = model.to(device)
                except RuntimeError as e:
                    if "meta tensor" in str(e).lower():
                        # meta tensor ë¬¸ì œ í•´ê²°
                        from torch.nn import Module
                        model = model.to_empty(device=device)
                        model.load_state_dict(model.state_dict(), strict=False)
                    else:
                        raise
                
                model.eval()
                _model_cache = model
                print(f"âœ… ë§ì¶¤ë²• êµì • ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return model
                
            finally:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)


def preprocess_text(
    raw_text: Optional[str], 
    use_typo_correction: bool = True,
    optimize_for_search: bool = True
) -> str:
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë§ì¶¤ë²• êµì •, ê³µë°± ì •ê·œí™”, ê²€ìƒ‰ ìµœì í™”)
    
    ë¦¬ì†ŒìŠ¤ ì ˆì•½: ì§§ì€ í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ ì˜ ì •ì œëœ í…ìŠ¤íŠ¸ëŠ” ë§ì¶¤ë²• êµì • ìŠ¤í‚µ
    
    Args:
        raw_text: ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
        use_typo_correction: ë§ì¶¤ë²• êµì • ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                           ì§§ì€ í…ìŠ¤íŠ¸(10ì ì´í•˜)ëŠ” ìë™ìœ¼ë¡œ ìŠ¤í‚µí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½
        optimize_for_search: ê²€ìƒ‰ ìµœì í™” ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                            ë¶ˆìš©ì–´ ì œê±°, í‚¤ì›Œë“œ ì •ê·œí™” ë“±
    """
    if not raw_text:
        return ""

    text = raw_text.strip()
    if not text:
        return ""

    corrected = text

    # ë§ì¶¤ë²• êµì • (ë¦¬ì†ŒìŠ¤ ì ˆì•½: ì§§ì€ í…ìŠ¤íŠ¸ë‚˜ í‚¤ì›Œë“œ í˜•íƒœëŠ” ìŠ¤í‚µ)
    if use_typo_correction:
        # ì§§ì€ í…ìŠ¤íŠ¸(10ì ì´í•˜)ë‚˜ í‚¤ì›Œë“œ í˜•íƒœëŠ” ë§ì¶¤ë²• êµì • ìŠ¤í‚µí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½
        should_correct = (
            len(text) > 10 and  # ê¸´ í…ìŠ¤íŠ¸ë§Œ êµì •
            not text.isdigit() and  # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ìŠ¤í‚µ
            ' ' in text  # ê³µë°±ì´ ìˆëŠ” ë¬¸ì¥ í˜•íƒœë§Œ êµì •
        )
        
        if should_correct:
            try:
                tokenizer = _load_tokenizer()
                model = _load_model()
                device = model.device

                # Typo correction via T5 model
                encoding = tokenizer(
                    f"ë§ì¶¤ë²•ì„ ê³ ì³ì£¼ì„¸ìš”: {text}",
                    return_tensors="pt",
                    truncation=True,
                    max_length=256,
                )
                input_ids = encoding.input_ids.to(device)
                attention_mask = encoding.attention_mask.to(device)

                with torch.no_grad():
                    output_ids = model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=128,
                        num_beams=3,  # 5 -> 3ìœ¼ë¡œ ì¤„ì—¬ì„œ ë¦¬ì†ŒìŠ¤ ì ˆì•½
                        early_stopping=True,
                    )

                corrected = tokenizer.decode(
                    output_ids[0],
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True,
                )
            except Exception as e:
                # ë§ì¶¤ë²• êµì • ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
                print(f"âš ï¸ ë§ì¶¤ë²• êµì • ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
                corrected = text

    # Remove unsupported characters (ê³µë°±ì€ ìœ ì§€)
    corrected = re.sub(r"[^ê°€-í£0-9a-zA-Zã„±-ã…ã…-ã…£ .,!?~]", "", corrected)

    # ê³µë°± ì •ê·œí™” (ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ, í•˜ì§€ë§Œ ê³µë°± ìì²´ëŠ” ìœ ì§€)
    corrected = re.sub(r'\s+', ' ', corrected)
    
    # Fix basic spacing & repeated emoticons
    corrected = re.sub(r'([ê°€-í£0-9a-zA-Z])([,.!?])', r'\1 \2', corrected)
    corrected = re.sub(r'([,.!?])([ê°€-í£0-9a-zA-Z])', r'\1 \2', corrected)
    corrected = emoticon_normalize(corrected, num_repeats=1)

    # ê²€ìƒ‰ ìµœì í™”: ë¶ˆìš©ì–´ ì œê±° ë° í‚¤ì›Œë“œ ì •ê·œí™”
    if optimize_for_search:
        words = corrected.split()
        # ë¶ˆìš©ì–´ ì œê±° (ê²€ìƒ‰ì— ë„ì›€ì´ ì•ˆ ë˜ëŠ” ë‹¨ì–´)
        filtered_words = [w for w in words if w not in STOPWORDS]
        corrected = ' '.join(filtered_words)

    return corrected.strip()


def expand_search_query(query: str) -> List[str]:
    """
    ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´/ìœ ì‚¬ì–´ ì¶”ê°€) - í˜ì‹ ì  ê°œì„ 
    
    ë¦¬ì†ŒìŠ¤ ì ˆì•½: ì‚¬ì „ ê¸°ë°˜ í™•ì¥ìœ¼ë¡œ LLM ì—†ì´ ë¹ ë¥´ê²Œ ì²˜ë¦¬
    ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ: ë™ì˜ì–´ë¥¼ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
    í‚¤ì›Œë“œ ì¡°í•© í™•ì¥: ìƒ‰ìƒ+ë¬¼í’ˆ ì¡°í•©ë„ í™•ì¥
    
    Args:
        query: ì›ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬
        
    Returns:
        í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ í¬í•¨)
    """
    if not query:
        return []
    
    expanded_queries = [query]  # ì›ë³¸ ì¿¼ë¦¬ í¬í•¨
    added_queries = set()  # ì¤‘ë³µ ë°©ì§€ìš©
    words = query.split()
    
    # 1. ë‹¨ì–´ë³„ ë™ì˜ì–´ í™•ì¥
    for word in words:
        word_lower = word.lower()
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
        if word in KEYWORD_EXPANSION:
            for synonym in KEYWORD_EXPANSION[word]:
                if synonym != word:
                    expanded_query = query.replace(word, synonym)
                    if expanded_query not in added_queries and expanded_query != query:
                        expanded_queries.append(expanded_query)
                        added_queries.add(expanded_query)
        
        # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰
        elif word_lower in KEYWORD_EXPANSION:
            for synonym in KEYWORD_EXPANSION[word_lower]:
                if synonym.lower() != word_lower:
                    expanded_query = query.replace(word, synonym)
                    if expanded_query not in added_queries and expanded_query != query:
                        expanded_queries.append(expanded_query)
                        added_queries.add(expanded_query)
    
    # 2. ìƒ‰ìƒ+ë¬¼í’ˆ ì¡°í•© í™•ì¥ (í˜ì‹ ì  ê°œì„ )
    # ì˜ˆ: "ë¹¨ê°„ìƒ‰ ìš´ë™í™”" -> "ë¹¨ê°• ìš´ë™í™”", "ë¹¨ê°•ìƒ‰ ì‹ ë°œ", "ë ˆë“œ ìš´ë™í™”" ë“±
    color_words = ['ë¹¨ê°„ìƒ‰', 'ë¹¨ê°•', 'ë¹¨ê°•ìƒ‰', 'ë ˆë“œ', 'ê²€ì€ìƒ‰', 'ê²€ì •', 'ê²€ì •ìƒ‰', 'ë¸”ë™',
                   'í°ìƒ‰', 'í•˜ì–‘', 'í•˜ì–€ìƒ‰', 'í™”ì´íŠ¸', 'íŒŒë€ìƒ‰', 'íŒŒë‘', 'íŒŒë‘ìƒ‰', 'ë¸”ë£¨',
                   'ë…¸ë€ìƒ‰', 'ë…¸ë‘', 'ë…¸ë‘ìƒ‰', 'ì˜ë¡œìš°', 'ì´ˆë¡ìƒ‰', 'ì´ˆë¡', 'ë…¹ìƒ‰', 'ê·¸ë¦°',
                   'íšŒìƒ‰', 'ê·¸ë ˆì´', 'ë² ì´ì§€ìƒ‰', 'ë² ì´ì§€', 'ê°ˆìƒ‰', 'ë¸Œë¼ìš´', 'ë¶„í™ìƒ‰', 'í•‘í¬']
    
    item_words = ['ìš´ë™í™”', 'ì‹ ë°œ', 'ì…”ì¸ ', 'í‹°ì…”ì¸ ', 'ê°€ë°©', 'ì§€ê°‘', 'ëª¨ì', 'ì¥ê°‘']
    
    # ìƒ‰ìƒê³¼ ë¬¼í’ˆì´ í•¨ê»˜ ìˆëŠ” ê²½ìš° ì¡°í•© í™•ì¥
    found_colors = [w for w in words if w in color_words or any(cw in w for cw in color_words)]
    found_items = [w for w in words if w in item_words or any(iw in w for iw in item_words)]
    
    if found_colors and found_items:
        # ìƒ‰ìƒ ë™ì˜ì–´ì™€ ë¬¼í’ˆ ë™ì˜ì–´ ì¡°í•©
        for color in found_colors:
            if color in KEYWORD_EXPANSION:
                color_synonyms = KEYWORD_EXPANSION[color]
            else:
                color_synonyms = [color]
            
            for item in found_items:
                if item in KEYWORD_EXPANSION:
                    item_synonyms = KEYWORD_EXPANSION[item]
                else:
                    item_synonyms = [item]
                
                # ìƒ‰ìƒ+ë¬¼í’ˆ ì¡°í•© ìƒì„±
                for cs in color_synonyms[:2]:  # ìµœëŒ€ 2ê°œë§Œ
                    for isyn in item_synonyms[:2]:  # ìµœëŒ€ 2ê°œë§Œ
                        if cs != color or isyn != item:
                            expanded_query = query.replace(color, cs).replace(item, isyn)
                            if expanded_query not in added_queries and expanded_query != query:
                                expanded_queries.append(expanded_query)
                                added_queries.add(expanded_query)
    
    # 3. íŒ¨í„´ í‚¤ì›Œë“œ í™•ì¥ (ì²´í¬, ìŠ¤íŠ¸ë¼ì´í”„ ë“±)
    pattern_words = ['ì²´í¬', 'ìŠ¤íŠ¸ë¼ì´í”„', 'ì¤„ë¬´ëŠ¬', 'ë„íŠ¸', 'í”Œë¼ì›Œ']
    for word in words:
        if word in pattern_words:
            # íŒ¨í„´ì´ í¬í•¨ëœ ê²½ìš° íŒ¨í„´ í‚¤ì›Œë“œ ì¶”ê°€
            if 'ì²´í¬' in word or 'ì²´í¬' in query:
                expanded_query = query + ' ì²´í¬'
                if expanded_query not in added_queries:
                    expanded_queries.append(expanded_query)
                    added_queries.add(expanded_query)
            if 'ìŠ¤íŠ¸ë¼ì´í”„' in word or 'ì¤„ë¬´ëŠ¬' in word or 'ì¤„' in word:
                expanded_query = query + ' ìŠ¤íŠ¸ë¼ì´í”„'
                if expanded_query not in added_queries:
                    expanded_queries.append(expanded_query)
                    added_queries.add(expanded_query)
    
    # ìµœëŒ€ 12ê°œë¡œ í™•ì¥ (ì›ë³¸ í¬í•¨) - ë” ë§ì€ ì¡°í•© ì‹œë„
    return expanded_queries[:12]


