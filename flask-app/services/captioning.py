import io
import os
import shutil
from functools import lru_cache
from typing import Optional

import torch
from PIL import Image
from transformers import (
    AutoProcessor,
    BitsAndBytesConfig,
)

# Qwen2.5-VL ëª¨ë¸ì„ ìœ„í•œ í´ë˜ìŠ¤ import
# transformers 4.51.3+ ì—ì„œ Qwen2_5_VLForConditionalGeneration ì§€ì›
try:
    from transformers import Qwen2_5_VLForConditionalGeneration
except ImportError:
    try:
        # ëŒ€ì²´ í´ë˜ìŠ¤ëª… ì‹œë„
        from transformers import Qwen2VLForConditionalGeneration as Qwen2_5_VLForConditionalGeneration
    except ImportError:
        # trust_remote_code=Trueë¡œ ìë™ í´ë˜ìŠ¤ ê°ì§€
        from transformers import AutoModelForCausalLM
        # ì‹¤ì œë¡œëŠ” AutoModelForVision2Seqë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, 
        # trust_remote_code=Trueë¡œ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•¨
        Qwen2_5_VLForConditionalGeneration = AutoModelForCausalLM

DEFAULT_MODEL_ID = os.getenv(
    "CAPTION_MODEL_ID",
    "Qwen/Qwen2.5-VL-7B-Instruct",
)


def _check_disk_space(min_free_gb: float = 5.0):
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì „ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ë° ì •ë¦¬"""
    try:
        stat = shutil.disk_usage("/")
        free_gb = stat.free / (1024**3)
        
        if free_gb < min_free_gb:
            print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ({free_gb:.2f}GB). ì •ë¦¬ ì¤‘...")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for tmp_dir in ["/tmp", "/var/tmp"]:
                if os.path.exists(tmp_dir):
                    try:
                        for item in os.listdir(tmp_dir):
                            item_path = os.path.join(tmp_dir, item)
                            try:
                                if os.path.isfile(item_path):
                                    os.remove(item_path)
                                elif os.path.isdir(item_path):
                                    shutil.rmtree(item_path)
                            except:
                                pass
                    except:
                        pass
            
            # ë‹¤ì‹œ í™•ì¸
            stat = shutil.disk_usage("/")
            free_gb_after = stat.free / (1024**3)
            print(f"âœ… ì •ë¦¬ ì™„ë£Œ: ì—¬ìœ  ê³µê°„ {free_gb_after:.2f}GB")
            
            if free_gb_after < min_free_gb:
                raise RuntimeError(
                    f"ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"í•„ìš”: {min_free_gb}GB ì´ìƒ, í˜„ì¬: {free_gb_after:.2f}GB. "
                    f"ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ í˜¸ìŠ¤íŠ¸ ë””ìŠ¤í¬ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”."
                )
        
        return True
    except RuntimeError:
        raise
    except Exception as e:
        print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰


@lru_cache(maxsize=1)
def _load_processor(model_id: str = DEFAULT_MODEL_ID) -> AutoProcessor:
    return AutoProcessor.from_pretrained(
        model_id, 
        trust_remote_code=True,
        use_fast=False,  # fast processor ê²½ê³  ë°©ì§€
    )


@lru_cache(maxsize=1)
def _load_model(model_id: str = DEFAULT_MODEL_ID):
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì „ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ì•½ 4GB í•„ìš”)
    _check_disk_space(min_free_gb=5.0)
    
    quantization = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    # Qwen2.5-VL ëª¨ë¸ ë¡œë“œ
    # trust_remote_code=Trueë¡œ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¥¼ ì„ íƒ
    print(f"ğŸ“¥ Qwen2.5-VL ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_id}")
    try:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_id,
            device_map="auto",
            trust_remote_code=True,
            quantization_config=quantization,
        )
    except Exception as e:
        # fallback: AutoModel ì‚¬ìš© (trust_remote_codeë¡œ ìë™ ê°ì§€)
        from transformers import AutoModel
        model = AutoModel.from_pretrained(
            model_id,
            device_map="auto",
            trust_remote_code=True,
            quantization_config=quantization,
        )
        # generate ë©”ì„œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        if not hasattr(model, 'generate'):
            raise RuntimeError(
                f"ë¡œë“œëœ ëª¨ë¸ì´ generate ë©”ì„œë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                f"transformers ë²„ì „ì„ 4.51.3 ì´ìƒìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”. ì›ë³¸ ì—ëŸ¬: {e}"
            )
    model.eval()
    return model


def generate_caption(
    image_bytes: bytes,
    prompt: Optional[str] = None,
) -> str:
    """
    Generate a Korean caption for the given image.
    ê²€ìƒ‰ ìµœì í™”ëœ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ.
    ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì€ ë™ì¼í•˜ì§€ë§Œ ê²€ìƒ‰ ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒë¨.
    """
    if not image_bytes:
        raise ValueError("ì´ë¯¸ì§€ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ê²€ìƒ‰ ìµœì í™” í”„ë¡¬í”„íŠ¸: í˜ì‹ ì  ê°œì„  - êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ ì¹œí™”ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    # ì‚¬ìš©ìê°€ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•  í‚¤ì›Œë“œë¥¼ ì •í™•í•˜ê²Œ í¬í•¨í•˜ë„ë¡ ê°•í™”
    prompt = prompt or (
        "ì´ ë¶„ì‹¤ë¬¼ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì¤˜. "
        "ë‹¤ìŒ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì¤˜ (ë³´ì´ëŠ” ê²ƒë§Œ, ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ): "
        "1. ìƒ‰ìƒ: ì£¼ìš” ìƒ‰ìƒ 2-3ê°œë¥¼ ì •í™•íˆ (ì˜ˆ: ë¹¨ê°„ìƒ‰, ê²€ì€ìƒ‰, í°ìƒ‰, íŒŒë€ìƒ‰, ë…¸ë€ìƒ‰, ì´ˆë¡ìƒ‰, íšŒìƒ‰, ë² ì´ì§€ìƒ‰, ê°ˆìƒ‰, ë¶„í™ìƒ‰ ë“±) "
        "2. íŒ¨í„´/ë¬´ëŠ¬: ì²´í¬, ìŠ¤íŠ¸ë¼ì´í”„, ë„íŠ¸, í”Œë¼ì›Œ, ë ˆì´ìŠ¤, í”„ë¦°íŠ¸, ì†”ë¦¬ë“œ(ë¬´ëŠ¬ì—†ìŒ) ë“± "
        "3. ë¬¼í’ˆ ì¢…ë¥˜: ì…”ì¸ , í‹°ì…”ì¸ , ìš´ë™í™”, ì‹ ë°œ, ì§€ê°‘, ê°€ë°©, í•¸ë“œí°, ë…¸íŠ¸ë¶, ì‹œê³„, ì•ˆê²½, ëª¨ì, ì¥ê°‘, ìš°ì‚° ë“± êµ¬ì²´ì ìœ¼ë¡œ "
        "4. ë¸Œëœë“œ: ë³´ì´ëŠ” ë¸Œëœë“œëª…ì´ ìˆìœ¼ë©´ ì •í™•íˆ (ë‚˜ì´í‚¤, ì•„ë””ë‹¤ìŠ¤, ìƒ˜ì†Œë‚˜ì´íŠ¸ ë“±) "
        "5. ì¬ì§ˆ: ê°€ì£½, ë‚˜ì¼ë¡ , ì½”íŠ¼, í´ë¦¬ì—ìŠ¤í„°, ì‹¤í¬, ë°ë‹˜, ìº”ë²„ìŠ¤, ë©”ì‰¬ ë“± "
        "6. íŠ¹ì§•: ë¡œê³ , ë¬¸ì–‘, ì†ìƒ, í¬ê¸°, ìŠ¤íƒ€ì¼ ë“± ëˆˆì— ë„ëŠ” íŠ¹ì§• "
        "í˜•ì‹: í‚¤ì›Œë“œë“¤ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´ (ì˜ˆ: 'ë¹¨ê°„ìƒ‰ ì²´í¬ ì…”ì¸  ì½”íŠ¼', 'ê²€ì€ìƒ‰ ë‚˜ì´í‚¤ ìš´ë™í™” ì—ì–´ë§¥ìŠ¤', 'í°ìƒ‰ ìŠ¤íŠ¸ë¼ì´í”„ í‹°ì…”ì¸ ') "
        "ì¤‘ìš”: ìƒ‰ìƒê³¼ íŒ¨í„´ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , ë¬¼í’ˆ ì¢…ë¥˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•´ì¤˜."
    )

    model_id = DEFAULT_MODEL_ID
    processor = _load_processor(model_id)
    model = _load_model(model_id)

    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt},
            ],
        }
    ]

    chat_text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = processor(
        text=[chat_text],
        images=[image],
        return_tensors="pt",
    ).to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=128,
            temperature=0.2,
            do_sample=False,
        )

    prompt_length = inputs["input_ids"].shape[1]
    generated_ids = generated_ids[:, prompt_length:]

    outputs = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )

    caption = outputs[0].strip()
    return caption


