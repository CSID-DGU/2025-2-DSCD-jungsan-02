import io
import os
import shutil
import threading
import fcntl
from functools import lru_cache
from typing import Optional

import torch
from PIL import Image
from transformers import (
    AutoProcessor,
    BitsAndBytesConfig,
)

# Qwen2.5-VL ëª¨ë¸ì„ ìœ„í•œ í´ë˜ìŠ¤ import
# transformers 4.51.3+ ì—ì„œ Qwen2_5_VLForConditionalGeneration ì§€ì›
try:
    from transformers import Qwen2_5_VLForConditionalGeneration
except ImportError:
    try:
        # ëŒ€ì²´ í´ë˜ìŠ¤ëª… ì‹œë„
        from transformers import Qwen2VLForConditionalGeneration as Qwen2_5_VLForConditionalGeneration
    except ImportError:
        # trust_remote_code=Trueë¡œ ìë™ í´ë˜ìŠ¤ ê°ì§€
        from transformers import AutoModelForCausalLM
        # ì‹¤ì œë¡œëŠ” AutoModelForVision2Seqë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, 
        # trust_remote_code=Trueë¡œ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•¨
        Qwen2_5_VLForConditionalGeneration = AutoModelForCausalLM

DEFAULT_MODEL_ID = os.getenv(
    "CAPTION_MODEL_ID",
    "Qwen/Qwen2.5-VL-7B-Instruct",
)


def _check_disk_space(min_free_gb: float = 5.0):
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì „ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ë° ì •ë¦¬"""
    try:
        stat = shutil.disk_usage("/")
        free_gb = stat.free / (1024**3)
        
        if free_gb < min_free_gb:
            print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ({free_gb:.2f}GB). ì •ë¦¬ ì¤‘...")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for tmp_dir in ["/tmp", "/var/tmp"]:
                if os.path.exists(tmp_dir):
                    try:
                        for item in os.listdir(tmp_dir):
                            item_path = os.path.join(tmp_dir, item)
                            try:
                                if os.path.isfile(item_path):
                                    os.remove(item_path)
                                elif os.path.isdir(item_path):
                                    shutil.rmtree(item_path)
                            except:
                                pass
                    except:
                        pass
            
            # ë‹¤ì‹œ í™•ì¸
            stat = shutil.disk_usage("/")
            free_gb_after = stat.free / (1024**3)
            print(f"âœ… ì •ë¦¬ ì™„ë£Œ: ì—¬ìœ  ê³µê°„ {free_gb_after:.2f}GB")
            
            if free_gb_after < min_free_gb:
                raise RuntimeError(
                    f"ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"í•„ìš”: {min_free_gb}GB ì´ìƒ, í˜„ì¬: {free_gb_after:.2f}GB. "
                    f"ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¥¼ í™•ì¸í•˜ê±°ë‚˜ í˜¸ìŠ¤íŠ¸ ë””ìŠ¤í¬ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”."
                )
        
        return True
    except RuntimeError:
        raise
    except Exception as e:
        print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰


# ì „ì—­ ë½ ë° ëª¨ë¸ ìºì‹œ (ì›Œì»¤ ê°„ ê³µìœ ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜)
_model_lock = threading.Lock()
_processor_cache = {}
_model_cache = {}

def _load_processor(model_id: str = DEFAULT_MODEL_ID) -> AutoProcessor:
    """í”„ë¡œì„¸ì„œ ë¡œë“œ (íŒŒì¼ ë½ìœ¼ë¡œ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€)"""
    if model_id in _processor_cache:
        return _processor_cache[model_id]
    
    with _model_lock:
        # ì´ì¤‘ ì²´í¬ (ë½ íšë“ í›„ ë‹¤ì‹œ í™•ì¸)
        if model_id in _processor_cache:
            return _processor_cache[model_id]
        
        processor = AutoProcessor.from_pretrained(
            model_id, 
            trust_remote_code=True,
            use_fast=False,  # fast processor ê²½ê³  ë°©ì§€
        )
        _processor_cache[model_id] = processor
        return processor


def _load_model(model_id: str = DEFAULT_MODEL_ID):
    """ëª¨ë¸ ë¡œë“œ (íŒŒì¼ ë½ìœ¼ë¡œ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë° GPU ë©”ëª¨ë¦¬ ì¶©ëŒ ë°©ì§€)"""
    if model_id in _model_cache:
        return _model_cache[model_id]
    
    # íŒŒì¼ ë½ ê²½ë¡œ (ì›Œì»¤ ê°„ ê³µìœ )
    lock_file_path = os.path.join(os.getenv("HF_HOME", "/tmp"), ".model_load_lock")
    os.makedirs(os.path.dirname(lock_file_path), exist_ok=True)
    
    with _model_lock:
        # ì´ì¤‘ ì²´í¬ (ë½ íšë“ í›„ ë‹¤ì‹œ í™•ì¸)
        if model_id in _model_cache:
            return _model_cache[model_id]
        
        # íŒŒì¼ ë½ìœ¼ë¡œ ë‹¤ë¥¸ ì›Œì»¤ì™€ì˜ ì¶©ëŒ ë°©ì§€
        with open(lock_file_path, 'w') as lock_file:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
            
            try:
                # ë‹¤ì‹œ í™•ì¸ (íŒŒì¼ ë½ íšë“ í›„)
                if model_id in _model_cache:
                    return _model_cache[model_id]
                
                # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì „ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ì•½ 4GB í•„ìš”)
                _check_disk_space(min_free_gb=5.0)
                
                # GPU ë©”ëª¨ë¦¬ í™•ì¸
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    allocated = torch.cuda.memory_allocated(0) / (1024**3)
                    free = gpu_memory - allocated
                    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ìƒíƒœ: ì „ì²´ {gpu_memory:.2f}GB, ì‚¬ìš© {allocated:.2f}GB, ì—¬ìœ  {free:.2f}GB")
                    
                    if free < 2.0:  # ìµœì†Œ 2GB í•„ìš”
                        print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ({free:.2f}GB < 2GB). ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬ ì¤‘...")
                        torch.cuda.empty_cache()
                        free = torch.cuda.get_device_properties(0).total_memory / (1024**3) - torch.cuda.memory_allocated(0) / (1024**3)
                        print(f"   ì •ë¦¬ í›„ ì—¬ìœ : {free:.2f}GB")
                
                quantization = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                )
                
                # Qwen2.5-VL ëª¨ë¸ ë¡œë“œ
                print(f"ğŸ“¥ Qwen2.5-VL ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_id}")
                try:
                    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        model_id,
                        device_map="auto",
                        trust_remote_code=True,
                        quantization_config=quantization,
                        low_cpu_mem_usage=True,  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
                    )
                except Exception as e:
                    # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU ì˜¤í”„ë¡œë“œ ì‹œë„
                    if "out of memory" in str(e).lower() or "CUDA" in str(e):
                        print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPU ì˜¤í”„ë¡œë“œ ì‹œë„: {e}")
                        quantization.llm_int8_enable_fp32_cpu_offload = True
                        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                            model_id,
                            device_map="auto",
                            trust_remote_code=True,
                            quantization_config=quantization,
                            low_cpu_mem_usage=True,
                        )
                    else:
                        # fallback: AutoModel ì‚¬ìš©
                        from transformers import AutoModel
                        model = AutoModel.from_pretrained(
                            model_id,
                            device_map="auto",
                            trust_remote_code=True,
                            quantization_config=quantization,
                            low_cpu_mem_usage=True,
                        )
                        if not hasattr(model, 'generate'):
                            raise RuntimeError(
                                f"ë¡œë“œëœ ëª¨ë¸ì´ generate ë©”ì„œë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                                f"transformers ë²„ì „ì„ 4.51.3 ì´ìƒìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”. ì›ë³¸ ì—ëŸ¬: {e}"
                            )
                
                model.eval()
                _model_cache[model_id] = model
                print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_id}")
                return model
                
            finally:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)


def generate_caption(
    image_bytes: bytes,
    prompt: Optional[str] = None,
) -> str:
    """
    Generate a Korean caption for the given image.
    ê²€ìƒ‰ ìµœì í™”ëœ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ.
    ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì€ ë™ì¼í•˜ì§€ë§Œ ê²€ìƒ‰ ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒë¨.
    """
    if not image_bytes:
        raise ValueError("ì´ë¯¸ì§€ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ê²€ìƒ‰ ìµœì í™” í”„ë¡¬í”„íŠ¸: í˜ì‹ ì  ê°œì„  - êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ ì¹œí™”ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    # ì‚¬ìš©ìê°€ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•  í‚¤ì›Œë“œë¥¼ ì •í™•í•˜ê²Œ í¬í•¨í•˜ë„ë¡ ê°•í™”
    prompt = prompt or (
        "ì´ ë¶„ì‹¤ë¬¼ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì¤˜. "
        "ë‹¤ìŒ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì¤˜ (ë³´ì´ëŠ” ê²ƒë§Œ, ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ): "
        "1. ìƒ‰ìƒ: ì£¼ìš” ìƒ‰ìƒ 2-3ê°œë¥¼ ì •í™•íˆ (ì˜ˆ: ë¹¨ê°„ìƒ‰, ê²€ì€ìƒ‰, í°ìƒ‰, íŒŒë€ìƒ‰, ë…¸ë€ìƒ‰, ì´ˆë¡ìƒ‰, íšŒìƒ‰, ë² ì´ì§€ìƒ‰, ê°ˆìƒ‰, ë¶„í™ìƒ‰ ë“±) "
        "2. íŒ¨í„´/ë¬´ëŠ¬: ì²´í¬, ìŠ¤íŠ¸ë¼ì´í”„, ë„íŠ¸, í”Œë¼ì›Œ, ë ˆì´ìŠ¤, í”„ë¦°íŠ¸, ì†”ë¦¬ë“œ(ë¬´ëŠ¬ì—†ìŒ) ë“± "
        "3. ë¬¼í’ˆ ì¢…ë¥˜: ì…”ì¸ , í‹°ì…”ì¸ , ìš´ë™í™”, ì‹ ë°œ, ì§€ê°‘, ê°€ë°©, í•¸ë“œí°, ë…¸íŠ¸ë¶, ì‹œê³„, ì•ˆê²½, ëª¨ì, ì¥ê°‘, ìš°ì‚° ë“± êµ¬ì²´ì ìœ¼ë¡œ "
        "4. ë¸Œëœë“œ: ë³´ì´ëŠ” ë¸Œëœë“œëª…ì´ ìˆìœ¼ë©´ ì •í™•íˆ (ë‚˜ì´í‚¤, ì•„ë””ë‹¤ìŠ¤, ìƒ˜ì†Œë‚˜ì´íŠ¸ ë“±) "
        "5. ì¬ì§ˆ: ê°€ì£½, ë‚˜ì¼ë¡ , ì½”íŠ¼, í´ë¦¬ì—ìŠ¤í„°, ì‹¤í¬, ë°ë‹˜, ìº”ë²„ìŠ¤, ë©”ì‰¬ ë“± "
        "6. íŠ¹ì§•: ë¡œê³ , ë¬¸ì–‘, ì†ìƒ, í¬ê¸°, ìŠ¤íƒ€ì¼ ë“± ëˆˆì— ë„ëŠ” íŠ¹ì§• "
        "í˜•ì‹: í‚¤ì›Œë“œë“¤ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´ (ì˜ˆ: 'ë¹¨ê°„ìƒ‰ ì²´í¬ ì…”ì¸  ì½”íŠ¼', 'ê²€ì€ìƒ‰ ë‚˜ì´í‚¤ ìš´ë™í™” ì—ì–´ë§¥ìŠ¤', 'í°ìƒ‰ ìŠ¤íŠ¸ë¼ì´í”„ í‹°ì…”ì¸ ') "
        "ì¤‘ìš”: ìƒ‰ìƒê³¼ íŒ¨í„´ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , ë¬¼í’ˆ ì¢…ë¥˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•´ì¤˜."
    )

    model_id = DEFAULT_MODEL_ID
    processor = _load_processor(model_id)
    model = _load_model(model_id)

    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt},
            ],
        }
    ]

    chat_text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = processor(
        text=[chat_text],
        images=[image],
        return_tensors="pt",
    ).to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=128,
            temperature=0.2,
            do_sample=False,
        )

    prompt_length = inputs["input_ids"].shape[1]
    generated_ids = generated_ids[:, prompt_length:]

    outputs = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )

    caption = outputs[0].strip()
    return caption


