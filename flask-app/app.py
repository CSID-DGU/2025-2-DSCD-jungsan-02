from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import faiss
import numpy as np
from datetime import datetime
import pickle
import torch
from typing import Optional
from sentence_transformers import SentenceTransformer
import threading
import concurrent.futures
import requests
import json
import time
from functools import lru_cache
import hashlib
import shutil
import subprocess
import fcntl  # íŒŒì¼ ì ê¸ˆìš©
import os

from services.captioning import generate_caption
from services.text_processing import preprocess_text, expand_search_query

app = Flask(__name__)
CORS(app)
app.config['MAX_CONTENT_LENGTH'] = 30 * 1024 * 1024  # 30MB upload limit

# ========== FAISS & ì„ë² ë”© ëª¨ë¸ ì„¤ì • ==========
FAISS_STORAGE_DIR = os.getenv("FAISS_STORAGE_DIR", "/app/faiss-data")
FAISS_INDEX_PATH = os.path.join(FAISS_STORAGE_DIR, 'faiss_index.idx')
FAISS_MAPPING_PATH = os.path.join(FAISS_STORAGE_DIR, 'id_mapping.pkl')
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "BAAI/bge-m3")
EMBEDDING_DIMENSION = int(os.getenv("EMBEDDING_DIMENSION", "768"))

# FAISS ì¸ë±ìŠ¤ íƒ€ì… ì„¤ì • (HNSW: ëŒ€ëŸ‰ ë°ì´í„° ê²€ìƒ‰ ìµœì í™”, Flat: ì†ŒëŸ‰ ë°ì´í„°)
FAISS_INDEX_TYPE = os.getenv("FAISS_INDEX_TYPE", "HNSW")  # "HNSW" or "Flat"
HNSW_M = int(os.getenv("HNSW_M", "32"))  # HNSW íŒŒë¼ë¯¸í„°: ì—°ê²° ìˆ˜ (16-64, ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
HNSW_EF_CONSTRUCTION = int(os.getenv("HNSW_EF_CONSTRUCTION", "200"))  # HNSW ë¹Œë“œ ì‹œ íƒìƒ‰ ë²”ìœ„
HNSW_EF_SEARCH = int(os.getenv("HNSW_EF_SEARCH", "128"))  # HNSW ê²€ìƒ‰ ì‹œ íƒìƒ‰ ë²”ìœ„ (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)

# ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ : ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
# IndexFlatIPì€ ë‚´ì  ê°’ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ * ë²¡í„° í¬ê¸°), ì •ê·œí™”ëœ ë²¡í„°ëŠ” 0~1 ë²”ìœ„
# BGE-M3ëŠ” ì •ê·œí™”ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ëŒ€ëµ 0.3~0.95 ë²”ìœ„
SIMILARITY_THRESHOLD = float(os.getenv("SIMILARITY_THRESHOLD", "0.3"))  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.3 = 30%)
MIN_RESULTS_TO_RETURN = int(os.getenv("MIN_RESULTS_TO_RETURN", "3"))  # ìµœì†Œ ë°˜í™˜ ê²°ê³¼ ìˆ˜

# ========== ì „ì—­ ë³€ìˆ˜ ==========
faiss_index = None
id_mapping = {}  # FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ -> MySQL item_id ë§¤í•‘
embedding_model: Optional[SentenceTransformer] = None
embedding_device = "cuda" if torch.cuda.is_available() else "cpu"
_faiss_initialized = False
_model_loaded = False
_faiss_lock = threading.Lock()  # FAISS ì¸ë±ìŠ¤ ì ‘ê·¼ ë™ê¸°í™”
_pending_save_count = 0  # ì €ì¥ ëŒ€ê¸° ì¤‘ì¸ ë²¡í„° ê°œìˆ˜
_save_batch_size = 10  # Nê°œë§ˆë‹¤ ì €ì¥ (ê°œë³„ APIìš©)

def check_and_free_disk_space(min_free_gb: float = 5.0):
    """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ë° í•„ìš”ì‹œ ì •ë¦¬"""
    try:
        # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        stat = shutil.disk_usage("/")
        free_gb = stat.free / (1024**3)
        total_gb = stat.total / (1024**3)
        used_percent = (stat.used / stat.total) * 100
        
        print(f"ğŸ’¾ ë””ìŠ¤í¬ ìƒíƒœ: ì „ì²´ {total_gb:.2f}GB, ì‚¬ìš© {used_percent:.1f}%, ì—¬ìœ  {free_gb:.2f}GB")
        
        if free_gb < min_free_gb:
            print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ({free_gb:.2f}GB < {min_free_gb}GB). ì •ë¦¬ ì‹œì‘...")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for tmp_dir in ["/tmp", "/var/tmp"]:
                if os.path.exists(tmp_dir):
                    try:
                        for item in os.listdir(tmp_dir):
                            item_path = os.path.join(tmp_dir, item)
                            try:
                                if os.path.isfile(item_path):
                                    os.remove(item_path)
                                elif os.path.isdir(item_path):
                                    shutil.rmtree(item_path)
                            except Exception as e:
                                pass  # ê¶Œí•œ ë¬¸ì œ ë“± ë¬´ì‹œ
                    except Exception as e:
                        pass
            
            # pip ìºì‹œ ì •ë¦¬
            pip_cache = os.path.expanduser("~/.cache/pip")
            if os.path.exists(pip_cache):
                try:
                    shutil.rmtree(pip_cache)
                except:
                    pass
            
            # Python ìºì‹œ ì •ë¦¬
            for root, dirs, files in os.walk("/usr/local/lib/python3.10"):
                for d in dirs:
                    if d == "__pycache__":
                        try:
                            shutil.rmtree(os.path.join(root, d))
                        except:
                            pass
            
            # ë‹¤ì‹œ í™•ì¸
            stat = shutil.disk_usage("/")
            free_gb_after = stat.free / (1024**3)
            print(f"âœ… ì •ë¦¬ ì™„ë£Œ: ì—¬ìœ  ê³µê°„ {free_gb_after:.2f}GB")
            
            if free_gb_after < min_free_gb:
                print(f"âŒ ê²½ê³ : ì—¬ì „íˆ ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ ({free_gb_after:.2f}GB)")
                return False
        
        return True
    except Exception as e:
        print(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

def _try_load_faiss_file() -> tuple[bool, Optional[object], Optional[dict]]:
    """FAISS íŒŒì¼ ë¡œë“œ ì‹œë„ (ì†ìƒëœ íŒŒì¼ ìë™ ì²˜ë¦¬)"""
    if not os.path.exists(FAISS_INDEX_PATH) or not os.path.exists(FAISS_MAPPING_PATH):
        return False, None, None
    
    try:
        # íŒŒì¼ í¬ê¸° í™•ì¸ (ì†ìƒëœ íŒŒì¼ ì‚¬ì „ ê°ì§€)
        index_size = os.path.getsize(FAISS_INDEX_PATH)
        if index_size == 0:
            print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ íŒŒì¼ í¬ê¸°ê°€ 0ì…ë‹ˆë‹¤. ì†ìƒëœ íŒŒì¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
            return False, None, None
        
        # íŒŒì¼ ì½ê¸° ì „ì— íŒŒì¼ì´ ì™„ì „íˆ ì“°ì—¬ì¡ŒëŠ”ì§€ í™•ì¸
        # íŒŒì¼ í¬ê¸°ê°€ ì¼ì • ì‹œê°„ ë™ì•ˆ ë³€í•˜ì§€ ì•Šìœ¼ë©´ ì™„ì „íˆ ì“°ì—¬ì§„ ê²ƒìœ¼ë¡œ ê°„ì£¼
        import time
        prev_size = index_size
        for _ in range(5):  # ìµœëŒ€ 5ë²ˆ í™•ì¸ (0.1ì´ˆ ê°„ê²©)
            time.sleep(0.1)
            current_size = os.path.getsize(FAISS_INDEX_PATH)
            if current_size != prev_size:
                # íŒŒì¼ì´ ì•„ì§ ì“°ì—¬ì§€ê³  ìˆìŒ
                prev_size = current_size
            else:
                break
        
        # íŒŒì¼ ì½ê¸° ì‹œë„ (ëª…ì‹œì ìœ¼ë¡œ ì˜ˆì™¸ ì²˜ë¦¬)
        try:
            faiss_index = faiss.read_index(FAISS_INDEX_PATH)
        except RuntimeError as e:
            # FAISS ì½ê¸° ì—ëŸ¬ (íŒŒì¼ ì†ìƒ ë“±)
            error_msg = str(e)
            if "read error" in error_msg or "ret == (size)" in error_msg:
                print(f"âŒ FAISS íŒŒì¼ ì½ê¸° ì—ëŸ¬ ê°ì§€: {error_msg}")
                print(f"   íŒŒì¼ í¬ê¸°: {os.path.getsize(FAISS_INDEX_PATH)} bytes")
                return False, None, None
            else:
                # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ RuntimeErrorëŠ” ì¬ë°œìƒ
                raise
        
        # ë§¤í•‘ íŒŒì¼ ì½ê¸°
        try:
            with open(FAISS_MAPPING_PATH, 'rb') as f:
                id_mapping = pickle.load(f)
        except (pickle.UnpicklingError, EOFError, IOError) as e:
            print(f"âŒ FAISS ë§¤í•‘ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False, None, None
        
        # ê²€ì¦: ì¸ë±ìŠ¤ì™€ ë§¤í•‘ì˜ ì¼ê´€ì„± í™•ì¸
        if faiss_index.ntotal != len(id_mapping):
            print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ì™€ ë§¤í•‘ ë¶ˆì¼ì¹˜: ì¸ë±ìŠ¤={faiss_index.ntotal}, ë§¤í•‘={len(id_mapping)}")
            return False, None, None
        
        return True, faiss_index, id_mapping
    except (RuntimeError, IOError, OSError, Exception) as e:
        # ì†ìƒëœ íŒŒì¼ ê°ì§€ (ëª¨ë“  ì˜ˆì™¸ íƒ€ì… ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬)
        print(f"âŒ FAISS íŒŒì¼ ì†ìƒ ê°ì§€: {type(e).__name__}: {e}")
        import traceback
        print(f"   ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
        return False, None, None

def initialize_faiss():
    """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰)
    
    ì¸ë±ìŠ¤ íƒ€ì…:
    - IndexFlatIP: ì •í™•í•˜ì§€ë§Œ ëŠë¦¼ (ì†ŒëŸ‰ ë°ì´í„°ìš©, < 10ë§Œê°œ)
    - IndexHNSWFlat: ë¹ ë¥´ê³  ì •í™•í•¨ (ëŒ€ëŸ‰ ë°ì´í„°ìš©, > 10ë§Œê°œ)
    """
    global faiss_index, id_mapping, _faiss_initialized
    
    if _faiss_initialized:
        return
    
    os.makedirs(FAISS_STORAGE_DIR, exist_ok=True)
    
    # íŒŒì¼ ì ê¸ˆ ê²½ë¡œ
    lock_file_path = os.path.join(FAISS_STORAGE_DIR, '.faiss_lock')
    
    # íŒŒì¼ ì ê¸ˆì„ ë¨¼ì € íšë“í•˜ì—¬ ë‹¤ë¥¸ ì›Œì»¤ì™€ì˜ ì¶©ëŒ ë°©ì§€
    # ì—¬ëŸ¬ ì›Œì»¤ê°€ ë™ì‹œì— ì‹œì‘ë  ë•Œë¥¼ ëŒ€ë¹„í•˜ì—¬ íƒ€ì„ì•„ì›ƒ ì„¤ì •
    lock_file = None
    lock_acquired = False
    max_retries = 10
    retry_delay = 0.5  # 0.5ì´ˆ
    
    for retry in range(max_retries):
        try:
            lock_file = open(lock_file_path, 'w')
            # ë…¼ë¸”ë¡œí‚¹ ì ê¸ˆ ì‹œë„ (LOCK_NB)
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            lock_acquired = True
            break
        except (IOError, OSError) as e:
            # ë‹¤ë¥¸ ì›Œì»¤ê°€ ì ê¸ˆì„ ë³´ìœ  ì¤‘ì´ë©´ ëŒ€ê¸°
            if lock_file:
                lock_file.close()
            if retry < max_retries - 1:
                print(f"â³ íŒŒì¼ ì ê¸ˆ ëŒ€ê¸° ì¤‘... (ì‹œë„ {retry + 1}/{max_retries})")
                time.sleep(retry_delay)
            else:
                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ë¸”ë¡œí‚¹ ì ê¸ˆ ì‹œë„
                print(f"âš ï¸ ë…¼ë¸”ë¡œí‚¹ ì ê¸ˆ ì‹¤íŒ¨, ë¸”ë¡œí‚¹ ì ê¸ˆ ì‹œë„...")
                lock_file = open(lock_file_path, 'w')
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
                lock_acquired = True
                break
    
    if not lock_acquired:
        raise RuntimeError("FAISS íŒŒì¼ ì ê¸ˆ íšë“ ì‹¤íŒ¨")
    
    try:
        # íŒŒì¼ ë¡œë“œ ì‹œë„ (ì†ìƒëœ íŒŒì¼ ìë™ ì²˜ë¦¬)
        success, loaded_index, loaded_mapping = _try_load_faiss_file()
        
        if success and loaded_index is not None and loaded_mapping is not None:
            # íŒŒì¼ ë¡œë“œ ì„±ê³µ
            faiss_index = loaded_index
            id_mapping = loaded_mapping
            index_type_name = type(faiss_index).__name__
            index_size = os.path.getsize(FAISS_INDEX_PATH)
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ: {faiss_index.ntotal}ê°œ ë²¡í„° (íƒ€ì…: {index_type_name}, í¬ê¸°: {index_size / 1024 / 1024:.2f}MB)")
            
            # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search ì„¤ì •
            if hasattr(faiss_index, 'hnsw'):
                faiss_index.hnsw.efSearch = HNSW_EF_SEARCH
                print(f"   HNSW íŒŒë¼ë¯¸í„° ì„¤ì •: ef_search={HNSW_EF_SEARCH}")
        else:
            # íŒŒì¼ì´ ì—†ê±°ë‚˜ ì†ìƒëœ ê²½ìš°
            if os.path.exists(FAISS_INDEX_PATH) or os.path.exists(FAISS_MAPPING_PATH):
                # ì†ìƒëœ íŒŒì¼ì¸ ê²½ìš° ë°±ì—… í›„ ì‚­ì œ
                print(f"ğŸ”„ ì†ìƒëœ íŒŒì¼ì„ ë°±ì—…í•˜ê³  ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                if os.path.exists(FAISS_INDEX_PATH):
                    backup_path = f"{FAISS_INDEX_PATH}.corrupted_{timestamp}"
                    try:
                        shutil.move(FAISS_INDEX_PATH, backup_path)
                        print(f"   ë°±ì—…: {backup_path}")
                    except Exception as e:
                        print(f"   ë°±ì—… ì‹¤íŒ¨, íŒŒì¼ ì‚­ì œ: {e}")
                        try:
                            os.remove(FAISS_INDEX_PATH)
                        except:
                            pass
                
                if os.path.exists(FAISS_MAPPING_PATH):
                    backup_path = f"{FAISS_MAPPING_PATH}.corrupted_{timestamp}"
                    try:
                        shutil.move(FAISS_MAPPING_PATH, backup_path)
                        print(f"   ë°±ì—…: {backup_path}")
                    except Exception as e:
                        print(f"   ë°±ì—… ì‹¤íŒ¨, íŒŒì¼ ì‚­ì œ: {e}")
                        try:
                            os.remove(FAISS_MAPPING_PATH)
                        except:
                            pass
                
                # ì†ìƒëœ íŒŒì¼ì´ ìˆì—ˆìœ¼ë¯€ë¡œ ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (ë©”ëª¨ë¦¬ìƒì—ì„œë§Œ, ì €ì¥ì€ ë‚˜ì¤‘ì—)
                if FAISS_INDEX_TYPE.upper() == "HNSW":
                    faiss_index = faiss.IndexHNSWFlat(EMBEDDING_DIMENSION, HNSW_M)
                    faiss_index.hnsw.efConstruction = HNSW_EF_CONSTRUCTION
                    faiss_index.hnsw.efSearch = HNSW_EF_SEARCH
                    print(f"âœ… HNSW FAISS ì¸ë±ìŠ¤ ìƒì„± (M={HNSW_M}, ef_construction={HNSW_EF_CONSTRUCTION}, ef_search={HNSW_EF_SEARCH})")
                else:
                    faiss_index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)
                    print("âœ… Flat FAISS ì¸ë±ìŠ¤ ìƒì„± (ì •í™•í•œ ê²€ìƒ‰)")
                
                id_mapping = {}
            else:
                # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°: CI/CD ì‹œ ì´ˆê¸°í™” ë°©ì§€
                # ë¹ˆ ì¸ë±ìŠ¤ë¥¼ ë©”ëª¨ë¦¬ì—ë§Œ ìƒì„± (ë””ìŠ¤í¬ì— ì €ì¥í•˜ì§€ ì•ŠìŒ)
                # CSV ì„í¬íŠ¸ API í˜¸ì¶œ ì‹œ ë°ì´í„°ê°€ ì¶”ê°€ë˜ë©´ ê·¸ë•Œ ì €ì¥ë¨
                print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ì¸ë±ìŠ¤ë¡œ ì‹œì‘í•©ë‹ˆë‹¤ (CSV ì„í¬íŠ¸ API í˜¸ì¶œ ì‹œ ë°ì´í„° ì¶”ê°€ë¨).")
                if FAISS_INDEX_TYPE.upper() == "HNSW":
                    faiss_index = faiss.IndexHNSWFlat(EMBEDDING_DIMENSION, HNSW_M)
                    faiss_index.hnsw.efConstruction = HNSW_EF_CONSTRUCTION
                    faiss_index.hnsw.efSearch = HNSW_EF_SEARCH
                else:
                    faiss_index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)
                id_mapping = {}
                print(f"   ì¸ë±ìŠ¤ íƒ€ì…: {FAISS_INDEX_TYPE}, ë²¡í„° ê°œìˆ˜: 0 (CSV ì„í¬íŠ¸ë¡œ ë°ì´í„° ì¶”ê°€ í•„ìš”)")
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (ë©”ëª¨ë¦¬ìƒì—ì„œë§Œ)
        print(f"âŒ FAISS ì´ˆê¸°í™” ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        # ì†ìƒëœ íŒŒì¼ ë°±ì—… ì‹œë„
        try:
            if os.path.exists(FAISS_INDEX_PATH):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = f"{FAISS_INDEX_PATH}.corrupted_{timestamp}"
                shutil.move(FAISS_INDEX_PATH, backup_path)
                print(f"   ì†ìƒëœ íŒŒì¼ ë°±ì—…: {backup_path}")
        except:
            pass
        
        # ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (ë©”ëª¨ë¦¬ìƒì—ì„œë§Œ, ë””ìŠ¤í¬ ì €ì¥ì€ ë‚˜ì¤‘ì—)
        if FAISS_INDEX_TYPE.upper() == "HNSW":
            faiss_index = faiss.IndexHNSWFlat(EMBEDDING_DIMENSION, HNSW_M)
            faiss_index.hnsw.efConstruction = HNSW_EF_CONSTRUCTION
            faiss_index.hnsw.efSearch = HNSW_EF_SEARCH
        else:
            faiss_index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)
        id_mapping = {}
        print(f"âœ… ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ì˜ˆì™¸ ë³µêµ¬, ë©”ëª¨ë¦¬ìƒì—ì„œë§Œ)")
    finally:
        # ì ê¸ˆ í•´ì œ
        if lock_file and lock_acquired:
            try:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
            except:
                pass
            lock_file.close()
    
    _faiss_initialized = True

def save_faiss():
    """FAISS ìŠ¤ëƒ…ìƒ· ì €ì¥ (íŒŒì¼ ì ê¸ˆìœ¼ë¡œ ë©€í‹° ì›Œì»¤ ì¶©ëŒ ë°©ì§€)"""
    global faiss_index, id_mapping
    
    if faiss_index is None:
        print("âš ï¸ FAISS ì¸ë±ìŠ¤ê°€ ì—†ì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    os.makedirs(FAISS_STORAGE_DIR, exist_ok=True)
    
    # íŒŒì¼ ì ê¸ˆ ê²½ë¡œ
    lock_file_path = os.path.join(FAISS_STORAGE_DIR, '.faiss_lock')
    
    # íŒŒì¼ ì ê¸ˆì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹° ì›Œì»¤ ë™ì‹œ ì ‘ê·¼ ë°©ì§€
    try:
        with open(lock_file_path, 'w') as lock_file:
            # ë°°íƒ€ì  ì ê¸ˆ íšë“ (ë‹¤ë¥¸ ì›Œì»¤ëŠ” ëŒ€ê¸°)
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
            
            try:
                # ì €ì¥ ì „ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 1GB í•„ìš”)
                stat = shutil.disk_usage(FAISS_STORAGE_DIR)
                free_gb = stat.free / (1024**3)
                if free_gb < 1.0:
                    print(f"âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±ìœ¼ë¡œ ì €ì¥ ì‹¤íŒ¨ (ì—¬ìœ : {free_gb:.2f}GB)")
                    raise RuntimeError(f"ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {free_gb:.2f}GB < 1.0GB")
                
                # ì›ìì  ì“°ê¸°: ì„ì‹œ íŒŒì¼ì— ì“°ê³  ì„±ê³µ í›„ ì›ë³¸ìœ¼ë¡œ ì´ë™
                temp_index_path = f"{FAISS_INDEX_PATH}.tmp"
                temp_mapping_path = f"{FAISS_MAPPING_PATH}.tmp"
                
                # ì„ì‹œ íŒŒì¼ì— ì €ì¥
                faiss.write_index(faiss_index, temp_index_path)
                
                with open(temp_mapping_path, 'wb') as f:
                    pickle.dump(id_mapping, f)
                
                # íŒŒì¼ í¬ê¸° í™•ì¸ (ì†ìƒ ë°©ì§€)
                temp_index_size = os.path.getsize(temp_index_path)
                if temp_index_size == 0:
                    raise RuntimeError("ì„ì‹œ ì¸ë±ìŠ¤ íŒŒì¼ í¬ê¸°ê°€ 0ì…ë‹ˆë‹¤")
                
                # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë°±ì—…
                if os.path.exists(FAISS_INDEX_PATH):
                    backup_path = f"{FAISS_INDEX_PATH}.backup"
                    try:
                        if os.path.exists(backup_path):
                            os.remove(backup_path)
                        shutil.copy2(FAISS_INDEX_PATH, backup_path)
                    except:
                        pass  # ë°±ì—… ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                
                # ì›ìì  ì´ë™ (ì„±ê³µ ì‹œì—ë§Œ ì›ë³¸ íŒŒì¼ êµì²´)
                shutil.move(temp_index_path, FAISS_INDEX_PATH)
                shutil.move(temp_mapping_path, FAISS_MAPPING_PATH)
                
                # ìµœì¢… í™•ì¸ (íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° ê²€ì¦)
                if not os.path.exists(FAISS_INDEX_PATH):
                    raise RuntimeError("ì¸ë±ìŠ¤ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
                final_size = os.path.getsize(FAISS_INDEX_PATH)
                if final_size == 0:
                    raise RuntimeError("ì €ì¥ëœ ì¸ë±ìŠ¤ íŒŒì¼ í¬ê¸°ê°€ 0ì…ë‹ˆë‹¤")
                
                if final_size != temp_index_size:
                    raise RuntimeError(f"íŒŒì¼ í¬ê¸° ë¶ˆì¼ì¹˜: ì˜ˆìƒ {temp_index_size}, ì‹¤ì œ {final_size}")
                
                print(f"ğŸ’¾ FAISS ì €ì¥ ì™„ë£Œ: {faiss_index.ntotal}ê°œ ë²¡í„° ({temp_index_size / 1024 / 1024:.2f}MB)")
                
            finally:
                # ì ê¸ˆ í•´ì œ (ìë™ìœ¼ë¡œ í•´ì œë¨)
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
                
    except BlockingIOError:
        # ë‹¤ë¥¸ ì›Œì»¤ê°€ ì‚¬ìš© ì¤‘ì´ë©´ ì €ì¥ ê±´ë„ˆëœ€
        print("âš ï¸ ë‹¤ë¥¸ ì›Œì»¤ê°€ íŒŒì¼ì„ ì‚¬ìš© ì¤‘ì´ì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_path in [f"{FAISS_INDEX_PATH}.tmp", f"{FAISS_MAPPING_PATH}.tmp"]:
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
        print(f"âŒ FAISS ì €ì¥ ì‹¤íŒ¨: {e}")
        raise

# ========== AI íŒ€ì´ êµ¬í˜„í•  í•¨ìˆ˜ë“¤ (í˜„ì¬ëŠ” ë”ë¯¸) ==========


def load_embedding_model() -> SentenceTransformer:
    """SentenceTransformer ëª¨ë¸ì„ 1íšŒ ë¡œë“œ"""
    global embedding_model, _model_loaded
    if embedding_model is None or not _model_loaded:
        # ëª¨ë¸ ë¡œë“œ ì „ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        check_and_free_disk_space(min_free_gb=2.0)
        print(f"ğŸ“¦ BGE ëª¨ë¸ ë¡œë“œ: {EMBEDDING_MODEL_NAME} (device={embedding_device})")
        embedding_model = SentenceTransformer(
            EMBEDDING_MODEL_NAME,
            device=embedding_device,
            trust_remote_code=True,
        )
        embedding_model.max_seq_length = 512
        _model_loaded = True
        print(f"âœ… BGE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return embedding_model

def describe_image_with_llava(image_bytes):
    """
    ì´ë¯¸ì§€ì—ì„œ ìì—°ì–´ ì„¤ëª… ìƒì„± (Qwen ê¸°ë°˜).
    ì›ë³¸ ìº¡ì…˜ì„ ë°˜í™˜í•˜ê³ , ì „ì²˜ë¦¬ëŠ” ë‚˜ì¤‘ì— í†µí•©ì ìœ¼ë¡œ ìˆ˜í–‰.
    """
    try:
        caption = generate_caption(image_bytes)
        # ì›ë³¸ ìº¡ì…˜ ë°˜í™˜ (ì „ì²˜ë¦¬ëŠ” ë‚˜ì¤‘ì— í†µí•©ì ìœ¼ë¡œ ìˆ˜í–‰)
        return caption.strip() if caption else ""
    except Exception as exc:
        print(f"âš ï¸ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹¤íŒ¨: {exc}")
        return ""

# ì„ë² ë”© ìºì‹œ (ìì£¼ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìºì‹±í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½)
_embedding_cache = {}
_embedding_cache_lock = threading.Lock()
EMBEDDING_CACHE_SIZE = 1000  # ìµœëŒ€ ìºì‹œ í¬ê¸°


def _get_text_hash(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ ìƒì„± (ìºì‹œ í‚¤ìš©)"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def create_embedding_vector(text: str, use_cache: bool = True):
    """
    BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    
    ë¦¬ì†ŒìŠ¤ ì ˆì•½: ìì£¼ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ëŠ” ìºì‹±í•˜ì—¬ ì¬ì‚¬ìš©
    
    Args:
        text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ (ì´ë¯¸ì§€ ë¬˜ì‚¬ + ì‚¬ìš©ì ì…ë ¥ ì„¤ëª…)
        use_cache (bool): ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
    Returns:
        numpy.ndarray: shape (EMBEDDING_DIMENSION,) ì„ë² ë”© ë²¡í„°
    """
    if not text or not text.strip():
        raise ValueError("ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ìºì‹œ í™•ì¸ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
    if use_cache:
        text_hash = _get_text_hash(text)
        with _embedding_cache_lock:
            if text_hash in _embedding_cache:
                return _embedding_cache[text_hash].copy()

    model = load_embedding_model()
    embedding = model.encode(
        [text],
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False,
        batch_size=1,  # ë°°ì¹˜ í¬ê¸° ëª…ì‹œ
    )[0].astype("float32")

    if embedding.shape[0] != EMBEDDING_DIMENSION:
        raise ValueError(
            f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ëŒ€ê°’={EMBEDDING_DIMENSION}, ì‹¤ì œê°’={embedding.shape[0]}"
        )

    # ìºì‹œ ì €ì¥ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
    if use_cache:
        with _embedding_cache_lock:
            # ìºì‹œ í¬ê¸° ì œí•œ (LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ ê²ƒ ì œê±°)
            if len(_embedding_cache) >= EMBEDDING_CACHE_SIZE:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (ê°„ë‹¨í•œ ë°©ì‹: ëœë¤ ì œê±°)
                oldest_key = next(iter(_embedding_cache))
                del _embedding_cache[oldest_key]
            _embedding_cache[text_hash] = embedding.copy()

    return embedding


def create_embedding_vectors_batch(texts: list[str], use_cache: bool = True):
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì )
    
    ë°°ì¹˜ ì²˜ë¦¬ë¡œ GPU í™œìš©ë„ í–¥ìƒ ë° ì²˜ë¦¬ ì†ë„ ê°œì„ 
    
    Args:
        texts (list[str]): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        use_cache (bool): ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
    Returns:
        list[numpy.ndarray]: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    if not texts:
        return []
    
    # ìºì‹œ í™•ì¸ ë° ë¯¸ìºì‹œëœ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
    uncached_texts = []
    uncached_indices = []
    cached_embeddings = {}
    
    if use_cache:
        for idx, text in enumerate(texts):
            if not text or not text.strip():
                cached_embeddings[idx] = None
                continue
            text_hash = _get_text_hash(text)
            with _embedding_cache_lock:
                if text_hash in _embedding_cache:
                    cached_embeddings[idx] = _embedding_cache[text_hash].copy()
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(idx)
    else:
        uncached_texts = [t for t in texts if t and t.strip()]
        uncached_indices = list(range(len(uncached_texts)))
    
    # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
    if uncached_texts:
        model = load_embedding_model()
        embeddings = model.encode(
            uncached_texts,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False,
            batch_size=32,  # ë°°ì¹˜ í¬ê¸° ìµœì í™”
        ).astype("float32")
        
        # ìºì‹œ ì €ì¥
        if use_cache:
            with _embedding_cache_lock:
                for text, embedding in zip(uncached_texts, embeddings):
                    text_hash = _get_text_hash(text)
                    if len(_embedding_cache) >= EMBEDDING_CACHE_SIZE:
                        oldest_key = next(iter(_embedding_cache))
                        del _embedding_cache[oldest_key]
                    _embedding_cache[text_hash] = embedding.copy()
        
        # ê²°ê³¼ ë§¤í•‘
        for idx, embedding in zip(uncached_indices, embeddings):
            cached_embeddings[idx] = embedding
    
    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ë°˜í™˜
    result = []
    for i in range(len(texts)):
        if i in cached_embeddings:
            result.append(cached_embeddings[i])
        else:
            result.append(None)
    
    return result

def create_embedding_from_image(image_bytes):
    """
    ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ê¸°ë°˜ ê²€ìƒ‰ìš©)
    
    TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
    
    Args:
        image_bytes (bytes): ê²€ìƒ‰ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ íŒŒì¼ì˜ ë°”ì´íŠ¸ ë°ì´í„°
        
    Returns:
        numpy.ndarray: shape (EMBEDDING_DIMENSION,) ì„ë² ë”© ë²¡í„°
        
    êµ¬í˜„ ê°€ì´ë“œ:
        1. CLIP ë˜ëŠ” ìœ ì‚¬í•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì‚¬ìš©
        2. ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        3. í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ê°™ì€ ê³µê°„ì— ë§¤í•‘ë˜ë„ë¡ ì²˜ë¦¬
        4. ì •ê·œí™” ì ìš©
    """
    caption = describe_image_with_llava(image_bytes)
    if not caption:
        raise ValueError("ì´ë¯¸ì§€ ìº¡ì…”ë‹ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return create_embedding_vector(caption)


def warmup_models():
    """ì„œë²„ ê¸°ë™ ì‹œ ì£¼ìš” ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì½œë“œìŠ¤íƒ€íŠ¸ë¥¼ ì¤„ì„."""
    global models_warmed
    if models_warmed:
        return
    try:
        print("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
        load_embedding_model()
        preprocess_text("ëª¨ë¸ ì›Œë°ì—…")
        models_warmed = True
        print("âœ… ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
    except Exception as exc:
        print(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {exc}")

models_warmed = False
# ê° ì›Œì»¤ ì‹œì‘ ì‹œ ëª¨ë¸ê³¼ FAISS ë¯¸ë¦¬ ë¡œë“œ
# ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì•±ì´ ì‹œì‘ë  ìˆ˜ ìˆë„ë¡ try-except ì²˜ë¦¬
try:
    initialize_faiss()
except Exception as e:
    print(f"âš ï¸ FAISS ì´ˆê¸°í™” ì‹¤íŒ¨ (ì•±ì€ ê³„ì† ì‹œì‘ë©ë‹ˆë‹¤): {e}")
    import traceback
    traceback.print_exc()
    # ë¹ˆ ì¸ë±ìŠ¤ë¡œ ì‹œì‘ (ì˜ˆì™¸ ë°œìƒ ì‹œ)
    if faiss_index is None:
        if FAISS_INDEX_TYPE.upper() == "HNSW":
            faiss_index = faiss.IndexHNSWFlat(EMBEDDING_DIMENSION, HNSW_M)
            faiss_index.hnsw.efConstruction = HNSW_EF_CONSTRUCTION
            faiss_index.hnsw.efSearch = HNSW_EF_SEARCH
        else:
            faiss_index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)
        id_mapping = {}
        _faiss_initialized = True
        print(f"âœ… ë¹ˆ FAISS ì¸ë±ìŠ¤ë¡œ ì‹œì‘í•©ë‹ˆë‹¤ (CSV ì„í¬íŠ¸ API í˜¸ì¶œ ì‹œ ë°ì´í„° ì¶”ê°€ë¨).")

try:
    warmup_models()
except Exception as e:
    print(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨ (ì•±ì€ ê³„ì† ì‹œì‘ë©ë‹ˆë‹¤): {e}")
    import traceback
    traceback.print_exc()


@app.route('/health')
def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'service': 'lostfound-ai-server',
        'faiss_vectors': faiss_index.ntotal if faiss_index else 0
    })

@app.route('/api/v1/embedding/create', methods=['POST'])
def create_embedding():
    """
    ë¶„ì‹¤ë¬¼ ë“±ë¡ ì‹œ ì„ë² ë”© ìƒì„± ë° FAISS ì €ì¥
    
    í”„ë¡œì„¸ìŠ¤:
    1. ì‚¬ìš©ìê°€ ë“±ë¡í•œ ì´ë¯¸ì§€ë¥¼ LLaVAë¡œ ë¶„ì„í•˜ì—¬ ìì—°ì–´ ì„¤ëª… ìƒì„±
    2. (ì´ë¯¸ì§€ ì„¤ëª… + ì‚¬ìš©ì ì…ë ¥ ì„¤ëª…)ì„ BGE-M3ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    3. ì„ë² ë”© ë²¡í„°ë¥¼ FAISS ì¸ë±ìŠ¤ì— ì €ì¥
    4. MySQL item_idì™€ FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ë§¤í•‘í•˜ì—¬ ì €ì¥
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - item_id: MySQL ë¶„ì‹¤ë¬¼ ID (í•„ìˆ˜)
    - description: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¶„ì‹¤ë¬¼ ì„¤ëª… (ì„ íƒ)
    - image: ë¶„ì‹¤ë¬¼ ì´ë¯¸ì§€ íŒŒì¼ (ì„ íƒ)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_id: ì›ë³¸ ID (í™•ì¸ìš©)
    - message: ê²°ê³¼ ë©”ì‹œì§€
    """
    try:
        item_id = request.form.get('item_id')
        item_name = request.form.get('item_name', '')  # ë¶„ì‹¤ë¬¼ ì œëª©
        raw_description = request.form.get('description', '')
        image_file = request.files.get('image')
        
        if not item_id:
            return jsonify({'success': False, 'message': 'item_id í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        # 1. ì´ë¯¸ì§€ ë¬˜ì‚¬ ìƒì„± (Qwen ê¸°ë°˜) - í˜ì‹ ì  ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        image_description = ""
        caption_failed = False
        if image_file:
            image_bytes = image_file.read()
            image_description = describe_image_with_llava(image_bytes)
            if not image_description:
                caption_failed = True
                if raw_description:
                    image_description = raw_description.strip()
                    print(f"âš ï¸ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹¤íŒ¨, ì›ë³¸ description ì‚¬ìš©: {image_description[:150]}...")
                else:
                    print(f"âš ï¸ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹¤íŒ¨, description ì—†ìŒ")
            else:
                print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {image_description[:150]}...")
        
        # 2. ìº¡ì…˜ + ë¶„ì‹¤ë¬¼ ì œëª© + ì‚¬ìš©ì ì„¤ëª… ê²°í•©
        #    í˜ì‹ ì  ê°œì„ : ì‚¬ìš©ì ì…ë ¥ë„ ê²€ìƒ‰ ìµœì í™” ì „ì²˜ë¦¬ ì ìš©
        parts = []
        if image_description:
            parts.append(image_description)
        if item_name and item_name.strip():
            parts.append(item_name.strip())
        if raw_description and raw_description.strip():
            # ì‚¬ìš©ì ì…ë ¥ë„ ê²€ìƒ‰ ìµœì í™” ì „ì²˜ë¦¬ ì ìš©
            processed_description = preprocess_text(
                raw_description.strip(),
                use_typo_correction=False,  # ì‚¬ìš©ì ì…ë ¥ì€ ë§ì¶¤ë²• êµì • ìŠ¤í‚µ
                optimize_for_search=True   # ê²€ìƒ‰ ìµœì í™” ì ìš©
            )
            if processed_description:
                parts.append(processed_description)
            else:
                parts.append(raw_description.strip())
        
        if not parts:
            return jsonify({'success': False, 'message': 'ì„¤ëª… ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ê²°í•©
        raw_full_text = " ".join(parts).strip()
        
        # 3. í†µí•© ì „ì²˜ë¦¬ (ê²€ìƒ‰ ì‹œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©)
        #    ì €ì¥ ì‹œì™€ ê²€ìƒ‰ ì‹œ ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì—¬ ì¼ê´€ì„± ë³´ì¥
        #    ë¦¬ì†ŒìŠ¤ ì ˆì•½: ë“±ë¡ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì •ì„ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
        final_text = preprocess_text(
            raw_full_text, 
            use_typo_correction=True,  # ë“±ë¡ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ì‚¬ìš©
            optimize_for_search=True   # ê²€ìƒ‰ ìµœì í™” ì ìš©
        )
        if not final_text or len(final_text.strip()) == 0:
            # ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš© (ê³µë°± ì œê±°ë§Œ)
            final_text = raw_full_text.strip()
        
        # ìµœì¢… ì„ë² ë”© í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥ (FAISS ì €ì¥ ì „)
        print(f"ğŸ“ [ì„ë² ë”© í…ìŠ¤íŠ¸] item_id={item_id}")
        print(f"   ìº¡ì…˜: {image_description[:200] if image_description else '(ì—†ìŒ)'}")
        print(f"   ì œëª©: {item_name if item_name else '(ì—†ìŒ)'}")
        print(f"   ì„¤ëª…: {raw_description[:200] if raw_description else '(ì—†ìŒ)'}")
        print(f"   ê²°í•©ëœ ì›ë³¸ í…ìŠ¤íŠ¸: {raw_full_text}")
        print(f"   ì „ì²˜ë¦¬ í›„ ìµœì¢… ì„ë² ë”© í…ìŠ¤íŠ¸: {final_text}")
        print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: ì›ë³¸={len(raw_full_text)}ì, ìµœì¢…={len(final_text)}ì")
        
        # 4. í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (BGE-M3 ì‚¬ìš©)
        #    ê²€ìƒ‰ ì‹œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
        embedding_vector = create_embedding_vector(final_text)
        
        # 4. FAISS ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€ (ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        should_save = False
        # íŒŒì¼ ì ê¸ˆì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹° ì›Œì»¤ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ FAISSì— ì¶”ê°€
        lock_file_path = os.path.join(FAISS_STORAGE_DIR, '.faiss_lock')
        os.makedirs(FAISS_STORAGE_DIR, exist_ok=True)
        
        with open(lock_file_path, 'w') as lock_file:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)  # ë°°íƒ€ì  ì ê¸ˆ
            try:
                with _faiss_lock:  # ê°™ì€ ì›Œì»¤ ë‚´ ìŠ¤ë ˆë“œ ë™ê¸°í™”ë„ ìœ ì§€
                    faiss_index.add(np.array([embedding_vector]))
                    faiss_idx = faiss_index.ntotal - 1
                    # 5. FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ â†” MySQL item_id ë§¤í•‘ ì €ì¥
                    id_mapping[faiss_idx] = int(item_id)
                    # ì €ì¥ ë¹ˆë„ ì œì–´: Nê°œë§ˆë‹¤ ì €ì¥í•˜ì—¬ ë””ìŠ¤í¬ I/O ìµœì í™”
                    global _pending_save_count
                    _pending_save_count += 1
                    if _pending_save_count >= _save_batch_size:
                        should_save = True
                        _pending_save_count = 0
            finally:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
        
        # 6. FAISS ì¸ë±ìŠ¤ ë° ë§¤í•‘ ì •ë³´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ì˜ì†ì„±)
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ì—¬ ë””ìŠ¤í¬ I/O ìµœì í™”
        if should_save:
            save_faiss()
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: item_id={item_id}, faiss_idx={faiss_idx}, ë²¡í„° ì°¨ì›={len(embedding_vector)}")
        
        return jsonify({
            'success': True,
            'item_id': int(item_id),
            'message': f'ì„ë² ë”© ìƒì„± ì™„ë£Œ (FAISS ì¸ë±ìŠ¤: {faiss_idx})'
        })
        
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/create-batch', methods=['POST'])
def create_embeddings_batch():
    """
    ë¶„ì‹¤ë¬¼ ë“±ë¡ ì‹œ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ë° FAISS ì €ì¥ (ì„±ëŠ¥ ìµœì í™”)
    
    ì—¬ëŸ¬ ì•„ì´í…œì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ ë° ì²˜ë¦¬ ì†ë„ í–¥ìƒ
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - items: ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ê° ì•„ì´í…œì€ ë‹¤ìŒ í•„ë“œ í¬í•¨)
      - item_id: MySQL ë¶„ì‹¤ë¬¼ ID (í•„ìˆ˜)
      - description: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¶„ì‹¤ë¬¼ ì„¤ëª… (ì„ íƒ)
      - image_url: ì´ë¯¸ì§€ URL (ì„ íƒ)
      - image: ì´ë¯¸ì§€ íŒŒì¼ (image_urlì´ ì—†ì„ ê²½ìš°, ì„ íƒ)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - results: ê° ì•„ì´í…œë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
      - item_id: ì›ë³¸ ID
      - success: ì„±ê³µ ì—¬ë¶€
      - message: ê²°ê³¼ ë©”ì‹œì§€
      - faiss_idx: FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ (ì„±ê³µ ì‹œ)
    """
    try:
        items_data = request.form.get('items')
        if not items_data:
            return jsonify({'success': False, 'message': 'items ë°ì´í„° í•„ìš”'}), 400
        
        try:
            items = json.loads(items_data)
        except json.JSONDecodeError:
            return jsonify({'success': False, 'message': 'items JSON íŒŒì‹± ì‹¤íŒ¨'}), 400
        
        if not isinstance(items, list) or len(items) == 0:
            return jsonify({'success': False, 'message': 'itemsëŠ” ë¹„ì–´ìˆì§€ ì•Šì€ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸
        if not _faiss_initialized:
            initialize_faiss()
        
        results = []
        successful_count = 0
        
        # ë°°ì¹˜ ì²˜ë¦¬: ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        def process_item(item):
            """ë‹¨ì¼ ì•„ì´í…œ ì²˜ë¦¬"""
            item_id = item.get('item_id')
            item_name = item.get('item_name', '')  # ë¶„ì‹¤ë¬¼ ì œëª©
            raw_description = item.get('description', '')
            image_url = item.get('image_url', '')
            
            if not item_id:
                return {
                    'item_id': None,
                    'success': False,
                    'message': 'item_id í•„ìš”'
                }
            
            try:
                # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ìº¡ì…”ë‹ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                image_description = ""
                caption_failed = False
                if image_url:
                    # Qwen 7B ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ê³ ë ¤: ì¬ì‹œë„ 1íšŒë¡œ ê°ì†Œ, íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                    max_retries = 1  # 2 -> 1ë¡œ ê°ì†Œí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½
                    for attempt in range(max_retries):
                        try:
                            response = requests.get(
                                image_url, 
                                timeout=(3, 10),  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•: (5,15) -> (3,10)
                                stream=True,
                                headers={'User-Agent': 'Mozilla/5.0'}  # ì¼ë¶€ ì„œë²„ì—ì„œ í•„ìš”
                            )
                            response.raise_for_status()
                            image_bytes = response.content
                            if len(image_bytes) > 20 * 1024 * 1024:  # 20MB ì œí•œ
                                raise ValueError("ì´ë¯¸ì§€ íŒŒì¼ì´ ë„ˆë¬´ í¼")
                            if len(image_bytes) == 0:
                                raise ValueError("ì´ë¯¸ì§€ íŒŒì¼ì´ ë¹„ì–´ìˆìŒ")
                            image_description = describe_image_with_llava(image_bytes)
                            if image_description:
                                break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
                            else:
                                caption_failed = True
                                break  # ìº¡ì…”ë‹ ì‹¤íŒ¨ ì‹œ ë£¨í”„ ì¢…ë£Œ
                        except Exception as e:
                            caption_failed = True
                            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ/ìº¡ì…”ë‹ ì‹¤íŒ¨ (item_id={item_id}): {e}")
                            # ì¬ì‹œë„ ì—†ì´ ë°”ë¡œ í…ìŠ¤íŠ¸ë¡œ ì§„í–‰í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½
                
                # ìº¡ì…”ë‹ ì‹¤íŒ¨ ì‹œ ì›ë³¸ description ì‚¬ìš©
                if caption_failed and not image_description and raw_description:
                    image_description = raw_description.strip()
                
                # 2. ìº¡ì…˜ + ë¶„ì‹¤ë¬¼ ì œëª© + ì‚¬ìš©ì ì„¤ëª… ê²°í•© ë° ì „ì²˜ë¦¬
                parts = []
                if image_description:
                    parts.append(image_description)
                if item_name and item_name.strip():
                    parts.append(item_name.strip())
                if raw_description and raw_description.strip():
                    parts.append(raw_description.strip())
                
                if not parts:
                    return {
                        'item_id': int(item_id),
                        'success': False,
                        'message': 'ì„¤ëª… ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤'
                    }
                
                raw_full_text = " ".join(parts).strip()
                final_text = preprocess_text(
                    raw_full_text,
                    use_typo_correction=True,
                    optimize_for_search=True
                )
                if not final_text or len(final_text.strip()) == 0:
                    final_text = raw_full_text.strip()
                
                # ìµœì¢… ì„ë² ë”© í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥ (FAISS ì €ì¥ ì „)
                print(f"ğŸ“ [ë°°ì¹˜ ì„ë² ë”© í…ìŠ¤íŠ¸] item_id={item_id}")
                print(f"   ìº¡ì…˜: {image_description[:200] if image_description else '(ì—†ìŒ)'}")
                print(f"   ì œëª©: {item_name if item_name else '(ì—†ìŒ)'}")
                print(f"   ì„¤ëª…: {raw_description[:200] if raw_description else '(ì—†ìŒ)'}")
                print(f"   ê²°í•©ëœ ì›ë³¸ í…ìŠ¤íŠ¸: {raw_full_text}")
                print(f"   ì „ì²˜ë¦¬ í›„ ìµœì¢… ì„ë² ë”© í…ìŠ¤íŠ¸: {final_text}")
                print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: ì›ë³¸={len(raw_full_text)}ì, ìµœì¢…={len(final_text)}ì")
                
                # 3. ì„ë² ë”© ë²¡í„° ìƒì„± (ìºì‹œ í™œìš©)
                embedding_vector = create_embedding_vector(final_text, use_cache=True)
                
                # 4. FAISS ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€ (ë©€í‹° ì›Œì»¤ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                faiss_idx = None
                before_count = faiss_index.ntotal
                # íŒŒì¼ ì ê¸ˆì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹° ì›Œì»¤ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ FAISSì— ì¶”ê°€
                lock_file_path = os.path.join(FAISS_STORAGE_DIR, '.faiss_lock')
                os.makedirs(FAISS_STORAGE_DIR, exist_ok=True)
                
                with open(lock_file_path, 'w') as lock_file:
                    fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)  # ë°°íƒ€ì  ì ê¸ˆ
                    try:
                        with _faiss_lock:  # ê°™ì€ ì›Œì»¤ ë‚´ ìŠ¤ë ˆë“œ ë™ê¸°í™”ë„ ìœ ì§€
                            faiss_index.add(np.array([embedding_vector]))
                            faiss_idx = faiss_index.ntotal - 1
                            id_mapping[faiss_idx] = int(item_id)
                    finally:
                        fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
                
                return {
                    'item_id': int(item_id),
                    'success': True,
                    'message': f'ì„ë² ë”© ìƒì„± ì™„ë£Œ',
                    'faiss_idx': faiss_idx
                }
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (item_id={item_id}): {str(e)}")
                return {
                    'item_id': int(item_id) if item_id else None,
                    'success': False,
                    'message': str(e)
                }
        
        # ìˆœì°¨ ì²˜ë¦¬ë¡œ ë³€ê²½ (ë©”ëª¨ë¦¬ ì•ˆì •ì„± í™•ë³´)
        # Qwen 7B ëª¨ë¸ì€ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ ë™ì‹œ ì²˜ë¦¬ ì‹œ OOM ë°œìƒ ê°€ëŠ¥
        # ìˆœì°¨ ì²˜ë¦¬ë¡œ ë³€ê²½í•˜ì—¬ ì•ˆì •ì„± í™•ë³´ (ì„±ëŠ¥ì€ ë‹¤ì†Œ ì €í•˜ë˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì•ˆì •ì„± ìš°ì„ )
        for item in items:
            result = process_item(item)
            results.append(result)
            if result.get('success'):
                successful_count += 1
            
            # ê° ì•„ì´í…œ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„ íƒì , ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬)
            # í•„ìš”ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ë©”ëª¨ë¦¬ ì •ë¦¬ í™œì„±í™”
            # if torch.cuda.is_available():
            #     torch.cuda.empty_cache()
        
        # FAISS ì¸ë±ìŠ¤ ë° ë§¤í•‘ ì •ë³´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ë°°ì¹˜ ì™„ë£Œ í›„ í•œ ë²ˆë§Œ)
        # ë°°ì¹˜ APIëŠ” í•­ìƒ ì €ì¥í•˜ì—¬ ë°ì´í„° ì†ì‹¤ ë°©ì§€
        with _faiss_lock:
            global _pending_save_count
            _pending_save_count = 0  # ë°°ì¹˜ ì €ì¥ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
        save_faiss()
        
        print(f"âœ… ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {successful_count}/{len(items)}ê°œ ì„±ê³µ")
        
        return jsonify({
            'success': True,
            'results': results,
            'summary': {
                'total': len(items),
                'successful': successful_count,
                'failed': len(items) - successful_count
            }
        })
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/search', methods=['POST'])
def search_embedding():
    """
    ìì—°ì–´ ê²€ìƒ‰: í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ê²€ìƒ‰
    
    í”„ë¡œì„¸ìŠ¤:
    1. ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ BGE-M3ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    2. FAISSì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ Top-K ê²€ìƒ‰
    3. ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ MySQL item_id ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - query: ìì—°ì–´ ê²€ìƒ‰ì–´ (í•„ìˆ˜)
      ì˜ˆ) "ì§€í•˜ì² ì—ì„œ ìƒì–´ë²„ë¦° ê²€ì€ ì§€ê°‘", "ê°•ë‚¨ì—­ì—ì„œ ë°œê²¬í•œ ì•„ì´í°"
    - top_k: ë°˜í™˜í•  ê°œìˆ˜ (ì„ íƒ, ê¸°ë³¸ 10)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_ids: ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ MySQL item_id ë¦¬ìŠ¤íŠ¸
    
    TODO: AI íŒ€ ì¶”ê°€ êµ¬í˜„ ì‚¬í•­
    - ë‚ ì§œ/ì¥ì†Œ í•„í„°ë§ ê°€ì¤‘ì¹˜ ì ìš©
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì‹œë§¨í‹±)
    """
    try:
        print(f"ğŸ” ê²€ìƒ‰ ìš”ì²­ ìˆ˜ì‹ : Content-Type={request.content_type}, Method={request.method}")
        
        data = request.get_json()
        if data is None:
            print(f"âŒ ìš”ì²­ ë³¸ë¬¸ì´ Noneì…ë‹ˆë‹¤. Content-Type: {request.content_type}")
            return jsonify({'success': False, 'message': 'ìš”ì²­ ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤'}), 400
        
        print(f"ğŸ“¥ ìš”ì²­ ë°ì´í„° ìˆ˜ì‹ : {data}")
        
        raw_query = data.get('query', '')
        top_k = data.get('top_k', 10)
        
        print(f"ğŸ” ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: raw_query='{raw_query}', top_k={top_k}")
        
        if not raw_query or not raw_query.strip():
            print(f"âŒ ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: raw_query='{raw_query}'")
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ë¦¬ì†ŒìŠ¤ ì ˆì•½: ê²€ìƒ‰ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ì„ íƒì )
        query = preprocess_text(
            raw_query,
            use_typo_correction=False,  # ê²€ìƒ‰ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ìŠ¤í‚µí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½
            optimize_for_search=True    # ê²€ìƒ‰ ìµœì í™” ì ìš©
        )
        if not query:
            query = raw_query.strip()
        
        print(f"ğŸ“ ì „ì²˜ë¦¬ í›„ ê²€ìƒ‰ì–´: '{query}' (ì›ë³¸: '{raw_query}')")
        
        if not query:
            print(f"âŒ ì „ì²˜ë¦¬ í›„ì—ë„ ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        # FAISS ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if faiss_index is None or faiss_index.ntotal == 0:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¹„ì–´ìˆìŒ: ntotal={faiss_index.ntotal if faiss_index else 0}, id_mapping={len(id_mapping)}")
            return jsonify({'success': True, 'item_ids': [], 'scores': []})
        
        print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒíƒœ: ntotal={faiss_index.ntotal}, id_mapping={len(id_mapping)}")
        
        # 1. ê²€ìƒ‰ì–´ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (BGE-M3 ì‚¬ìš©, ìºì‹œ í™œìš©)
        print(f"ğŸ”„ ê²€ìƒ‰ì–´ ì„ë² ë”© ë²¡í„° ë³€í™˜ ì‹œì‘: query='{query}'")
        query_vector = create_embedding_vector(query, use_cache=True)
        print(f"âœ… ì„ë² ë”© ë²¡í„° ìƒì„± ì™„ë£Œ: shape={query_vector.shape}")
        
        # 2. FAISSì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        # top_këŠ” ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜ë¡œë§Œ ì‚¬ìš© (ìƒí•œì„ )
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë¥¼ ìµœëŒ€ top_kê°œê¹Œì§€ ë°˜í™˜
        k = min(max(top_k * 3, top_k + 50), faiss_index.ntotal)  # ì¶©ë¶„íˆ ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        if k == 0:
            print(f"âŒ k=0: top_k={top_k}, ntotal={faiss_index.ntotal}")
            return jsonify({'success': True, 'item_ids': [], 'scores': []})
        
        print(f"ğŸ“Š ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: top_k={top_k} (ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜), k={k} (ê²€ìƒ‰ ë²”ìœ„), ntotal={faiss_index.ntotal}, ì„ê³„ê°’={SIMILARITY_THRESHOLD}")
        
        # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search íŒŒë¼ë¯¸í„° ì„¤ì •
        if hasattr(faiss_index, 'hnsw'):
            faiss_index.hnsw.efSearch = max(HNSW_EF_SEARCH, k * 2)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        print(f"ğŸ” FAISS ê²€ìƒ‰ ì‹¤í–‰: k={k}, ntotal={faiss_index.ntotal}")
        distances, indices = faiss_index.search(np.array([query_vector]), k)
        print(f"âœ… FAISS ê²€ìƒ‰ ì™„ë£Œ: distances shape={distances.shape}, indices shape={indices.shape}")
        
        valid_results = len([idx for idx in indices[0] if int(idx) != -1])
        if valid_results == 0:
            print(f"âŒ FAISS ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: k={k}, ntotal={faiss_index.ntotal}, id_mapping={len(id_mapping)}")
            return jsonify({'success': True, 'item_ids': [], 'scores': []})
        
        print(f"ğŸ“Š ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼: {valid_results}ê°œ")
        
        # FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ â†’ MySQL item_id ë³€í™˜ ë° ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ (top_këŠ” ìµœëŒ€ ê°œìˆ˜ë¡œë§Œ ì‚¬ìš©)
        item_ids = []
        scores = []
        threshold_passed = 0
        threshold_failed = 0
        mapping_missing = 0
        
        for idx, dist in zip(indices[0], distances[0]):
            # top_kë¥¼ ì´ˆê³¼í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨ (ìµœëŒ€ ê°œìˆ˜ ì œí•œ)
            if len(item_ids) >= top_k:
                break
                
            if int(idx) != -1:
                if int(idx) in id_mapping:
                    score = float(dist)  # IndexFlatIPì´ë¯€ë¡œ ë‚´ì  ê°’ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                    # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í¬í•¨ (ë™ì  í•„í„°ë§)
                    # BGE-M3ëŠ” ì •ê·œí™”ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë‚´ì  ê°’ì€ ëŒ€ëµ 0.3~0.95 ë²”ìœ„
                    if score >= SIMILARITY_THRESHOLD:
                        threshold_passed += 1
                        item_ids.append(id_mapping[int(idx)])
                        scores.append(score)
                    else:
                        threshold_failed += 1
                        # ì„ê³„ê°’ ë¯¸ë§Œì¸ ê²½ìš° ë¡œê·¸ (ë””ë²„ê¹…ìš©, ì²˜ìŒ ëª‡ ê°œë§Œ)
                        if threshold_failed <= 5:
                            print(f"   ì„ê³„ê°’ ë¯¸ë§Œ: item_id={id_mapping[int(idx)]}, score={score:.4f} < {SIMILARITY_THRESHOLD}")
                else:
                    mapping_missing += 1
        
        # ì•ˆì „ì¥ì¹˜: ëª¨ë“  scoresë¥¼ Python floatë¡œ ê°•ì œ ë³€í™˜ (numpy íƒ€ì… ë°©ì§€)
        safe_scores = []
        for s in scores:
            try:
                safe_scores.append(float(s))  # numpy float32, float64 ë“± ëª¨ë“  ìˆ«ì íƒ€ì…ì„ Python floatë¡œ ë³€í™˜
            except (TypeError, ValueError):
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ 0.0ìœ¼ë¡œ ëŒ€ì²´ (ì•ˆì „ì¥ì¹˜)
                safe_scores.append(0.0)
        
        result = {
            'success': True,
            'item_ids': item_ids,  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ ë°˜í™˜ (ìµœëŒ€ top_kê°œ)
            'scores': safe_scores
        }
        
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ ë° ì‘ë‹µ ë°˜í™˜: item_ids={len(result['item_ids'])}, scores={len(result['scores'])}")
        print(f"   ì„ê³„ê°’ í†µê³¼: {threshold_passed}ê°œ, ì„ê³„ê°’ ë¯¸ë§Œ: {threshold_failed}ê°œ, ë§¤í•‘ ì—†ìŒ: {mapping_missing}ê°œ")
        if result['item_ids']:
            print(f"   ìƒìœ„ 5ê°œ item_ids: {result['item_ids'][:5]}")
        if result['scores']:
            print(f"   ìƒìœ„ 5ê°œ scores: {[f'{s:.4f}' for s in result['scores'][:5]]}")
        print(f"   ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜(top_k): {top_k}, ì‹¤ì œ ë°˜í™˜: {len(result['item_ids'])}ê°œ (ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒë§Œ)")
        
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        import traceback
        print(f"   ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/search-by-image', methods=['POST'])
def search_by_image():
    """
    ì´ë¯¸ì§€ ê¸°ë°˜ ê²€ìƒ‰: ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ê²€ìƒ‰
    
    í”„ë¡œì„¸ìŠ¤:
    1. ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ LLaVA ë˜ëŠ” CLIPìœ¼ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    2. FAISSì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
    3. ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œëŒ€ë¡œ MySQL item_id ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - image: ê²€ìƒ‰ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ íŒŒì¼ (í•„ìˆ˜)
    - top_k: ë°˜í™˜í•  ê°œìˆ˜ (ì„ íƒ, ê¸°ë³¸ 10)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_ids: ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ MySQL item_id ë¦¬ìŠ¤íŠ¸
    
    TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
    - create_embedding_from_image() í•¨ìˆ˜ êµ¬í˜„
    - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (CLIP ë“±) ì‚¬ìš©
    """
    try:
        image_file = request.files.get('image')
        top_k = int(request.form.get('top_k', 10))
        
        if not image_file:
            return jsonify({'success': False, 'message': 'ì´ë¯¸ì§€ íŒŒì¼ í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        if faiss_index is None or faiss_index.ntotal == 0:
            return jsonify({'success': True, 'item_ids': [], 'scores': []})
        
        # 1. ì´ë¯¸ì§€ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        #    AI íŒ€: create_embedding_from_image() í•¨ìˆ˜ êµ¬í˜„ í•„ìš”
        image_bytes = image_file.read()
        try:
            query_vector = create_embedding_from_image(image_bytes)
        except ValueError as err:
            return jsonify({'success': False, 'message': str(err)}), 400
        
        # 2. FAISSì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
        # top_këŠ” ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜ë¡œë§Œ ì‚¬ìš© (ìƒí•œì„ )
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë¥¼ ìµœëŒ€ top_kê°œê¹Œì§€ ë°˜í™˜
        k = min(max(top_k * 3, top_k + 50), faiss_index.ntotal)  # ì¶©ë¶„íˆ ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        
        # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search íŒŒë¼ë¯¸í„° ì„¤ì •
        if hasattr(faiss_index, 'hnsw'):
            faiss_index.hnsw.efSearch = max(HNSW_EF_SEARCH, k * 2)
        
        distances, indices = faiss_index.search(np.array([query_vector]), k)
        
        # 3. FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ â†’ MySQL item_id ë³€í™˜ ë° ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ (top_këŠ” ìµœëŒ€ ê°œìˆ˜ë¡œë§Œ ì‚¬ìš©)
        item_ids = []
        scores = []
        threshold_passed = 0
        threshold_failed = 0
        
        for idx, dist in zip(indices[0], distances[0]):
            # top_kë¥¼ ì´ˆê³¼í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨ (ìµœëŒ€ ê°œìˆ˜ ì œí•œ)
            if len(item_ids) >= top_k:
                break
                
            if int(idx) != -1 and int(idx) in id_mapping:
                score = float(dist)  # numpy float32ë¥¼ Python floatë¡œ ëª…ì‹œì  ë³€í™˜
                # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í¬í•¨ (ë™ì  í•„í„°ë§)
                if score >= SIMILARITY_THRESHOLD:
                    threshold_passed += 1
                    item_ids.append(id_mapping[int(idx)])
                    scores.append(score)
                else:
                    threshold_failed += 1
        
        # ì•ˆì „ì¥ì¹˜: ëª¨ë“  scoresë¥¼ Python floatë¡œ ê°•ì œ ë³€í™˜ (numpy íƒ€ì… ë°©ì§€)
        safe_scores = []
        for s in scores:
            try:
                safe_scores.append(float(s))  # numpy float32, float64 ë“± ëª¨ë“  ìˆ«ì íƒ€ì…ì„ Python floatë¡œ ë³€í™˜
            except (TypeError, ValueError):
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ 0.0ìœ¼ë¡œ ëŒ€ì²´ (ì•ˆì „ì¥ì¹˜)
                safe_scores.append(0.0)
        
        print(f"ğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰ ì™„ë£Œ: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜(top_k)={top_k}, ì‹¤ì œ ë°˜í™˜={len(item_ids)}ê°œ")
        print(f"   ì„ê³„ê°’ í†µê³¼: {threshold_passed}ê°œ, ì„ê³„ê°’ ë¯¸ë§Œ: {threshold_failed}ê°œ, ì„ê³„ê°’={SIMILARITY_THRESHOLD}")
        
        return jsonify({
            'success': True,
            'item_ids': item_ids,  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ ë°˜í™˜ (ìµœëŒ€ top_kê°œ)
            'scores': safe_scores
        })
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/search-with-filters', methods=['POST'])
def search_with_filters():
    """
    í•„í„°ë§ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ê³ ê¸‰ ê²€ìƒ‰
    
    í”„ë¡œì„¸ìŠ¤:
    1. ìì—°ì–´ ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ í›„ë³´ ì¶”ì¶œ
    2. ë‚ ì§œ, ì¥ì†Œ í•„í„°ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì ìš©
    3. ì¬ì •ë ¬í•˜ì—¬ ê²°ê³¼ ë°˜í™˜
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - query: ìì—°ì–´ ê²€ìƒ‰ì–´ (í•„ìˆ˜)
    - top_k: ë°˜í™˜í•  ê°œìˆ˜ (ì„ íƒ, ê¸°ë³¸ 10)
    - filters: í•„í„°ë§ ì¡°ê±´ (ì„ íƒ)
      - location: ì¥ì†Œ (ì˜ˆ: "ê°•ë‚¨ì—­")
      - start_date: ì‹œì‘ ë‚ ì§œ (ì˜ˆ: "2025-01-01")
      - end_date: ì¢…ë£Œ ë‚ ì§œ (ì˜ˆ: "2025-01-31")
    - weights: ê°€ì¤‘ì¹˜ ì„¤ì • (ì„ íƒ)
      - semantic: ì‹œë§¨í‹± ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ 0.7)
      - location: ì¥ì†Œ ì¼ì¹˜ ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ 0.2)
      - date: ë‚ ì§œ ì¼ì¹˜ ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ 0.1)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_ids: ì¬ì •ë ¬ëœ MySQL item_id ë¦¬ìŠ¤íŠ¸
    
    TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
    - í•„í„°ë§ ë¡œì§ êµ¬í˜„
    - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ
    - Springì—ì„œ ë©”íƒ€ë°ì´í„° í•¨ê»˜ ì „ë‹¬ë°›ëŠ” ë°©ì‹ ê³ ë ¤
    
    ì°¸ê³ :
    í˜„ì¬ëŠ” ê¸°ë³¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•˜ë©°, í•„í„°ë§ì€ Spring ë‹¨ì—ì„œ ì²˜ë¦¬ë¨
    í–¥í›„ AI ë‹¨ì—ì„œ í•„í„°ë§ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ê³ ê¸‰ ê²€ìƒ‰ êµ¬í˜„ ê°€ëŠ¥
    """
    try:
        data = request.get_json()
        raw_query = data.get('query', '')
        if not raw_query or not raw_query.strip():
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
        query = preprocess_text(
            raw_query,
            use_typo_correction=False,  # ê²€ìƒ‰ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ìŠ¤í‚µ
            optimize_for_search=True
        )
        if not query:
            query = raw_query.strip()
        
        top_k = data.get('top_k', 10)
        filters = data.get('filters', {})
        weights = data.get('weights', {
            'semantic': 0.7,
            'location': 0.2,
            'date': 0.1
        })
        
        if not query:
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        if faiss_index is None or faiss_index.ntotal == 0:
            return jsonify({'success': True, 'item_ids': []})
        
        # 1. ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰ (ìºì‹œ í™œìš©)
        query_vector = create_embedding_vector(query, use_cache=True)
        k = min(top_k * 3, faiss_index.ntotal)  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        
        # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search íŒŒë¼ë¯¸í„° ì„¤ì •
        if hasattr(faiss_index, 'hnsw'):
            faiss_index.hnsw.efSearch = max(HNSW_EF_SEARCH, k * 2)
        
        distances, indices = faiss_index.search(np.array([query_vector]), k)
        
        # 2. ì´ˆê¸° í›„ë³´ ì¶”ì¶œ
        item_ids = []
        for idx in indices[0]:
            if int(idx) in id_mapping:
                item_ids.append(id_mapping[int(idx)])
        
        # TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
        # 3. í•„í„°ë§ ê°€ì¤‘ì¹˜ ì ìš© ë° ì¬ì •ë ¬
        # - Springì—ì„œ ê° itemì˜ ë©”íƒ€ë°ì´í„°(ì¥ì†Œ, ë‚ ì§œ ë“±)ë¥¼ ë°›ì•„ì•¼ í•¨
        # - ë˜ëŠ” Flaskì—ì„œ ë³„ë„ DB ì—°ê²°í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        # - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìŠ¤ì½”ì–´ ê³„ì‚°: score = w1*sim + w2*loc_match + w3*date_match
        # - ìŠ¤ì½”ì–´ ê¸°ë°˜ ì¬ì •ë ¬
        
        print(f"ğŸ” í•„í„°ë§ ê²€ìƒ‰ ì™„ë£Œ: query='{query[:30]}...', ê²°ê³¼={len(item_ids[:top_k])}ê°œ")
        
        return jsonify({
            'success': True,
            'item_ids': item_ids[:top_k]
        })
        
    except Exception as e:
        print(f"âŒ í•„í„°ë§ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/admin/sync-with-db', methods=['POST'])
def sync_faiss_with_db():
    """
    Admin API: DBì™€ FAISS ë™ê¸°í™”
    
    DBì—ëŠ” ì—†ì§€ë§Œ FAISSì—ëŠ” ìˆëŠ” í•­ëª©ë“¤ì„ ì°¾ì•„ì„œ FAISSì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    (ê³ ì•„ ë°ì´í„° ì •ë¦¬)
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - db_item_ids: DBì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ëª¨ë“  item_id ë¦¬ìŠ¤íŠ¸ (í•„ìˆ˜)
      ì˜ˆ: [1, 2, 3, 5, 7, ...]
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - total_faiss_items: FAISSì— ìˆëŠ” ì „ì²´ í•­ëª© ìˆ˜
    - total_db_items: DBì— ìˆëŠ” ì „ì²´ í•­ëª© ìˆ˜
    - orphaned_items: FAISSì—ëŠ” ìˆì§€ë§Œ DBì—ëŠ” ì—†ëŠ” í•­ëª© ë¦¬ìŠ¤íŠ¸
    - deleted_count: ì‹¤ì œë¡œ ì‚­ì œëœ í•­ëª© ìˆ˜
    """
    try:
        data = request.get_json()
        db_item_ids = data.get('db_item_ids', [])
        
        if not isinstance(db_item_ids, list):
            return jsonify({'success': False, 'message': 'db_item_idsëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸
        if not _faiss_initialized:
            initialize_faiss()
        
        if faiss_index is None:
            return jsonify({
                'success': False,
                'message': 'FAISS ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
            }), 500
        
        # DB item_idë¥¼ setìœ¼ë¡œ ë³€í™˜ (ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´)
        db_item_set = set(int(item_id) for item_id in db_item_ids)
        
        # FAISSì— ìˆëŠ” ëª¨ë“  item_id ì¶”ì¶œ
        faiss_item_ids = set(id_mapping.values())
        
        # ê³ ì•„ ë°ì´í„° ì°¾ê¸°: FAISSì—ëŠ” ìˆì§€ë§Œ DBì—ëŠ” ì—†ëŠ” í•­ëª©ë“¤
        orphaned_item_ids = faiss_item_ids - db_item_set
        
        print(f"ğŸ” ë™ê¸°í™” ì‹œì‘:")
        print(f"   DB í•­ëª© ìˆ˜: {len(db_item_set)}")
        print(f"   FAISS í•­ëª© ìˆ˜: {len(faiss_item_ids)}")
        print(f"   ê³ ì•„ ë°ì´í„°: {len(orphaned_item_ids)}ê°œ")
        
        if len(orphaned_item_ids) == 0:
            print("âœ… ë™ê¸°í™” ì™„ë£Œ: ê³ ì•„ ë°ì´í„° ì—†ìŒ")
            return jsonify({
                'success': True,
                'total_faiss_items': len(faiss_item_ids),
                'total_db_items': len(db_item_set),
                'orphaned_items': [],
                'deleted_count': 0,
                'message': 'ê³ ì•„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë™ê¸°í™” ì™„ë£Œ.'
            })
        
        # ê³ ì•„ ë°ì´í„°ë¥¼ FAISSì—ì„œ ì‚­ì œ
        deleted_count = 0
        deleted_item_ids = []
        
        with _faiss_lock:
            for orphaned_item_id in orphaned_item_ids:
                try:
                    # í•´ë‹¹ item_idì˜ faiss_idx ì°¾ê¸°
                    faiss_indices_to_delete = [k for k, v in id_mapping.items() if v == orphaned_item_id]
                    
                    if len(faiss_indices_to_delete) == 0:
                        continue
                    
                    # FAISS ì¸ë±ìŠ¤ì—ì„œ ë²¡í„° ì‚­ì œ
                    if hasattr(faiss_index, 'remove_ids'):
                        try:
                            ids_to_remove = np.array(faiss_indices_to_delete, dtype=np.int64)
                            faiss_index.remove_ids(ids_to_remove)
                            deleted_count += len(faiss_indices_to_delete)
                        except Exception as e:
                            print(f"âš ï¸  FAISS ë²¡í„° ì‚­ì œ ì‹¤íŒ¨ (item_id={orphaned_item_id}): {str(e)}")
                    
                    # id_mappingì—ì„œ ì œê±°
                    for faiss_idx in faiss_indices_to_delete:
                        if faiss_idx in id_mapping:
                            del id_mapping[faiss_idx]
                    
                    deleted_item_ids.append(orphaned_item_id)
                    
                except Exception as e:
                    print(f"âš ï¸  í•­ëª© ì‚­ì œ ì‹¤íŒ¨ (item_id={orphaned_item_id}): {str(e)}")
                    continue
        
        # ì˜ì†ì„± ì €ì¥
        save_faiss()
        
        print(f"âœ… ë™ê¸°í™” ì™„ë£Œ: {deleted_count}ê°œ ë²¡í„° ì‚­ì œë¨ (ê³ ì•„ ë°ì´í„° {len(orphaned_item_ids)}ê°œ)")
        
        return jsonify({
            'success': True,
            'total_faiss_items': len(faiss_item_ids),
            'total_db_items': len(db_item_set),
            'orphaned_items': sorted(list(orphaned_item_ids)),
            'deleted_count': deleted_count,
            'deleted_item_ids': sorted(deleted_item_ids),
            'message': f'{deleted_count}ê°œ ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
        
    except Exception as e:
        print(f"âŒ ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/delete/<int:item_id>', methods=['DELETE'])
def delete_embedding(item_id):
    """
    ë¶„ì‹¤ë¬¼ ì‚­ì œ ì‹œ ì„ë² ë”© ì œê±°
    
    í”„ë¡œì„¸ìŠ¤:
    1. id_mappingì—ì„œ í•´ë‹¹ item_idì˜ faiss_idx ì°¾ê¸°
    2. FAISS ì¸ë±ìŠ¤ì—ì„œ ë²¡í„° ë¬¼ë¦¬ì  ì‚­ì œ (HNSWì˜ ê²½ìš° remove_ids ì‚¬ìš©)
    3. id_mappingì—ì„œ ì œê±°
    4. ì˜ì†ì„± ì €ì¥
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - item_id: ì‚­ì œí•  ë¶„ì‹¤ë¬¼ì˜ MySQL ID
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - deleted_count: ì‚­ì œëœ ë²¡í„° ê°œìˆ˜
    """
    try:
        with _faiss_lock:
            # 1. id_mappingì—ì„œ í•´ë‹¹ item_idì˜ faiss_idx ì°¾ê¸°
            faiss_indices_to_delete = [k for k, v in id_mapping.items() if v == item_id]
            
            if len(faiss_indices_to_delete) == 0:
                print(f"âš ï¸  ì‚­ì œ ì‹œë„: item_id={item_id}, í•˜ì§€ë§Œ ë§¤í•‘ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ)")
                return jsonify({'success': True, 'deleted_count': 0})
            
            # 2. FAISS ì¸ë±ìŠ¤ì—ì„œ ë²¡í„° ë¬¼ë¦¬ì  ì‚­ì œ
            deleted_count = 0
            if hasattr(faiss_index, 'remove_ids'):
                # HNSW ì¸ë±ìŠ¤: remove_ids() ë©”ì„œë“œ ì‚¬ìš©
                try:
                    # FAISSì˜ remove_idsëŠ” numpy arrayë¥¼ ë°›ìŒ
                    ids_to_remove = np.array(faiss_indices_to_delete, dtype=np.int64)
                    faiss_index.remove_ids(ids_to_remove)
                    deleted_count = len(faiss_indices_to_delete)
                    print(f"ğŸ—‘ï¸  FAISSì—ì„œ ë²¡í„° ì‚­ì œ ì™„ë£Œ: item_id={item_id}, faiss_indices={faiss_indices_to_delete}")
                except Exception as e:
                    print(f"âš ï¸  FAISS ë²¡í„° ì‚­ì œ ì‹¤íŒ¨ (id_mappingë§Œ ì œê±°): {str(e)}")
                    # FAISS ì‚­ì œ ì‹¤íŒ¨í•´ë„ id_mappingì€ ì œê±°
            else:
                # Flat ì¸ë±ìŠ¤: ì§ì ‘ ì‚­ì œ ë¶ˆê°€ëŠ¥, id_mappingì—ì„œë§Œ ì œê±°
                print(f"âš ï¸  Flat ì¸ë±ìŠ¤ëŠ” ì§ì ‘ ì‚­ì œ ë¶ˆê°€ëŠ¥, id_mappingì—ì„œë§Œ ì œê±°: item_id={item_id}")
            
            # 3. id_mappingì—ì„œ ì œê±°
            for faiss_idx in faiss_indices_to_delete:
                if faiss_idx in id_mapping:
                    del id_mapping[faiss_idx]
                    deleted_count = max(deleted_count, 1)  # ìµœì†Œ 1ê°œëŠ” ì‚­ì œë¨
        
        # 4. ì˜ì†ì„± ì €ì¥
        save_faiss()
        
        print(f"ğŸ—‘ï¸  ì‚­ì œ ì™„ë£Œ: item_id={item_id}, ì œê±°ëœ ë²¡í„°={deleted_count}ê°œ (FAISS ì¸ë±ìŠ¤: {faiss_index.ntotal}ê°œ ë‚¨ìŒ)")
        
        return jsonify({'success': True, 'deleted_count': deleted_count})
        
    except Exception as e:
        print(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'message': str(e)}), 500

# Gunicornìœ¼ë¡œ ì‹¤í–‰í•  ë•Œë„ FAISS ì´ˆê¸°í™”ëŠ” warmup_models()ì—ì„œ ìˆ˜í–‰ë¨
# ê° ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ê°€ ì‹œì‘ë  ë•Œë§ˆë‹¤ ì´ˆê¸°í™”ë¨

if __name__ == '__main__':
    # ê°œë°œ ëª¨ë“œì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ì‚¬ìš©
    port = int(os.environ.get('PORT', 5001))
    debug = os.environ.get('DEBUG', 'False').lower() == 'true'
    
    app.run(
        host='0.0.0.0',
        port=port,
        debug=debug
    )
