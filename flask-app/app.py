from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import faiss
import numpy as np
from datetime import datetime
import pickle
import torch
from typing import Optional
from sentence_transformers import SentenceTransformer
import threading
import concurrent.futures
import requests
import json
import time
from functools import lru_cache
import hashlib

from services.captioning import generate_caption
from services.text_processing import preprocess_text, expand_search_query

app = Flask(__name__)
CORS(app)
app.config['MAX_CONTENT_LENGTH'] = 30 * 1024 * 1024  # 30MB upload limit

# ========== FAISS & ì„ë² ë”© ëª¨ë¸ ì„¤ì • ==========
FAISS_STORAGE_DIR = os.getenv("FAISS_STORAGE_DIR", "/app/faiss-data")
FAISS_INDEX_PATH = os.path.join(FAISS_STORAGE_DIR, 'faiss_index.idx')
FAISS_MAPPING_PATH = os.path.join(FAISS_STORAGE_DIR, 'id_mapping.pkl')
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "BAAI/bge-m3")
EMBEDDING_DIMENSION = int(os.getenv("EMBEDDING_DIMENSION", "768"))

# FAISS ì¸ë±ìŠ¤ íƒ€ì… ì„¤ì • (HNSW: ëŒ€ëŸ‰ ë°ì´í„° ê²€ìƒ‰ ìµœì í™”, Flat: ì†ŒëŸ‰ ë°ì´í„°)
FAISS_INDEX_TYPE = os.getenv("FAISS_INDEX_TYPE", "HNSW")  # "HNSW" or "Flat"
HNSW_M = int(os.getenv("HNSW_M", "32"))  # HNSW íŒŒë¼ë¯¸í„°: ì—°ê²° ìˆ˜ (16-64, ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
HNSW_EF_CONSTRUCTION = int(os.getenv("HNSW_EF_CONSTRUCTION", "200"))  # HNSW ë¹Œë“œ ì‹œ íƒìƒ‰ ë²”ìœ„
HNSW_EF_SEARCH = int(os.getenv("HNSW_EF_SEARCH", "128"))  # HNSW ê²€ìƒ‰ ì‹œ íƒìƒ‰ ë²”ìœ„ (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)

# ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ : ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
# IndexFlatIPì€ ë‚´ì  ê°’ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ * ë²¡í„° í¬ê¸°), ì •ê·œí™”ëœ ë²¡í„°ëŠ” 0~1 ë²”ìœ„
# BGE-M3ëŠ” ì •ê·œí™”ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ëŒ€ëµ 0.3~0.95 ë²”ìœ„
SIMILARITY_THRESHOLD = float(os.getenv("SIMILARITY_THRESHOLD", "0.3"))  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.3 = 30%)
MIN_RESULTS_TO_RETURN = int(os.getenv("MIN_RESULTS_TO_RETURN", "3"))  # ìµœì†Œ ë°˜í™˜ ê²°ê³¼ ìˆ˜

# ========== ì „ì—­ ë³€ìˆ˜ ==========
faiss_index = None
id_mapping = {}  # FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ -> MySQL item_id ë§¤í•‘
embedding_model: Optional[SentenceTransformer] = None
embedding_device = "cuda" if torch.cuda.is_available() else "cpu"
_faiss_initialized = False
_model_loaded = False
_faiss_lock = threading.Lock()  # FAISS ì¸ë±ìŠ¤ ì ‘ê·¼ ë™ê¸°í™”

def initialize_faiss():
    """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰)
    
    ì¸ë±ìŠ¤ íƒ€ì…:
    - IndexFlatIP: ì •í™•í•˜ì§€ë§Œ ëŠë¦¼ (ì†ŒëŸ‰ ë°ì´í„°ìš©, < 10ë§Œê°œ)
    - IndexHNSWFlat: ë¹ ë¥´ê³  ì •í™•í•¨ (ëŒ€ëŸ‰ ë°ì´í„°ìš©, > 10ë§Œê°œ)
    """
    global faiss_index, id_mapping, _faiss_initialized
    
    if _faiss_initialized:
        return
    
    os.makedirs(FAISS_STORAGE_DIR, exist_ok=True)
    
    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_MAPPING_PATH):
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ (íƒ€ì… ìë™ ê°ì§€)
        faiss_index = faiss.read_index(FAISS_INDEX_PATH)
        with open(FAISS_MAPPING_PATH, 'rb') as f:
            id_mapping = pickle.load(f)
        index_type_name = type(faiss_index).__name__
        print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ: {faiss_index.ntotal}ê°œ ë²¡í„° (íƒ€ì…: {index_type_name})")
        
        # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search ì„¤ì •
        if hasattr(faiss_index, 'hnsw'):
            faiss_index.hnsw.efSearch = HNSW_EF_SEARCH
            print(f"   HNSW íŒŒë¼ë¯¸í„° ì„¤ì •: ef_search={HNSW_EF_SEARCH}")
        
        # ì¸ë±ìŠ¤ íƒ€ì… ë¶ˆì¼ì¹˜ ê²½ê³  (ì„¤ì •ê³¼ ë‹¤ë¥¸ ê²½ìš°)
        if FAISS_INDEX_TYPE.upper() == "HNSW" and "Flat" in index_type_name and "HNSW" not in index_type_name:
            print(f"âš ï¸ ê²½ê³ : ì„¤ì •ì€ HNSWì´ì§€ë§Œ ê¸°ì¡´ ì¸ë±ìŠ¤ëŠ” {index_type_name}ì…ë‹ˆë‹¤.")
            print(f"   ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ì›í•˜ë©´ ê¸°ì¡´ íŒŒì¼ì„ ì‚­ì œí•˜ì„¸ìš”.")
        elif FAISS_INDEX_TYPE.upper() == "FLAT" and "HNSW" in index_type_name:
            print(f"âš ï¸ ê²½ê³ : ì„¤ì •ì€ Flatì´ì§€ë§Œ ê¸°ì¡´ ì¸ë±ìŠ¤ëŠ” {index_type_name}ì…ë‹ˆë‹¤.")
            print(f"   ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ì›í•˜ë©´ ê¸°ì¡´ íŒŒì¼ì„ ì‚­ì œí•˜ì„¸ìš”.")
    else:
        # ì¸ë±ìŠ¤ íƒ€ì…ì— ë”°ë¼ ì„ íƒ
        if FAISS_INDEX_TYPE.upper() == "HNSW":
            # HNSW ì¸ë±ìŠ¤: ëŒ€ëŸ‰ ë°ì´í„° ê²€ìƒ‰ ìµœì í™” (ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ)
            # IndexHNSWFlat: ë‚´ì  ê¸°ë°˜ + HNSW ê·¸ë˜í”„ êµ¬ì¡°
            faiss_index = faiss.IndexHNSWFlat(EMBEDDING_DIMENSION, HNSW_M)
            faiss_index.hnsw.efConstruction = HNSW_EF_CONSTRUCTION
            faiss_index.hnsw.efSearch = HNSW_EF_SEARCH
            print(f"âœ… HNSW FAISS ì¸ë±ìŠ¤ ìƒì„± (M={HNSW_M}, ef_construction={HNSW_EF_CONSTRUCTION}, ef_search={HNSW_EF_SEARCH})")
        else:
            # Flat ì¸ë±ìŠ¤: ì •í™•í•œ ê²€ìƒ‰ (ì†ŒëŸ‰ ë°ì´í„°ìš©)
            faiss_index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)
            print("âœ… Flat FAISS ì¸ë±ìŠ¤ ìƒì„± (ì •í™•í•œ ê²€ìƒ‰)")
        
        id_mapping = {}
    
    _faiss_initialized = True

def save_faiss():
    """FAISS ìŠ¤ëƒ…ìƒ· ì €ì¥"""
    os.makedirs(FAISS_STORAGE_DIR, exist_ok=True)
    faiss.write_index(faiss_index, FAISS_INDEX_PATH)
    with open(FAISS_MAPPING_PATH, 'wb') as f:
        pickle.dump(id_mapping, f)
    print(f"ğŸ’¾ FAISS ì €ì¥: {faiss_index.ntotal}ê°œ ë²¡í„°")

# ========== AI íŒ€ì´ êµ¬í˜„í•  í•¨ìˆ˜ë“¤ (í˜„ì¬ëŠ” ë”ë¯¸) ==========


def load_embedding_model() -> SentenceTransformer:
    """SentenceTransformer ëª¨ë¸ì„ 1íšŒ ë¡œë“œ"""
    global embedding_model, _model_loaded
    if embedding_model is None or not _model_loaded:
        print(f"ğŸ“¦ BGE ëª¨ë¸ ë¡œë“œ: {EMBEDDING_MODEL_NAME} (device={embedding_device})")
        embedding_model = SentenceTransformer(
            EMBEDDING_MODEL_NAME,
            device=embedding_device,
            trust_remote_code=True,
        )
        embedding_model.max_seq_length = 512
        _model_loaded = True
        print(f"âœ… BGE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return embedding_model

def describe_image_with_llava(image_bytes):
    """
    ì´ë¯¸ì§€ì—ì„œ ìì—°ì–´ ì„¤ëª… ìƒì„± (Qwen ê¸°ë°˜).
    ì›ë³¸ ìº¡ì…˜ì„ ë°˜í™˜í•˜ê³ , ì „ì²˜ë¦¬ëŠ” ë‚˜ì¤‘ì— í†µí•©ì ìœ¼ë¡œ ìˆ˜í–‰.
    """
    try:
        caption = generate_caption(image_bytes)
        # ì›ë³¸ ìº¡ì…˜ ë°˜í™˜ (ì „ì²˜ë¦¬ëŠ” ë‚˜ì¤‘ì— í†µí•©ì ìœ¼ë¡œ ìˆ˜í–‰)
        return caption.strip() if caption else ""
    except Exception as exc:
        print(f"âš ï¸ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹¤íŒ¨: {exc}")
        return ""

# ì„ë² ë”© ìºì‹œ (ìì£¼ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìºì‹±í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½)
_embedding_cache = {}
_embedding_cache_lock = threading.Lock()
EMBEDDING_CACHE_SIZE = 1000  # ìµœëŒ€ ìºì‹œ í¬ê¸°


def _get_text_hash(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ ìƒì„± (ìºì‹œ í‚¤ìš©)"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def create_embedding_vector(text: str, use_cache: bool = True):
    """
    BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    
    ë¦¬ì†ŒìŠ¤ ì ˆì•½: ìì£¼ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ëŠ” ìºì‹±í•˜ì—¬ ì¬ì‚¬ìš©
    
    Args:
        text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ (ì´ë¯¸ì§€ ë¬˜ì‚¬ + ì‚¬ìš©ì ì…ë ¥ ì„¤ëª…)
        use_cache (bool): ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
    Returns:
        numpy.ndarray: shape (EMBEDDING_DIMENSION,) ì„ë² ë”© ë²¡í„°
    """
    if not text or not text.strip():
        raise ValueError("ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ìºì‹œ í™•ì¸ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
    if use_cache:
        text_hash = _get_text_hash(text)
        with _embedding_cache_lock:
            if text_hash in _embedding_cache:
                return _embedding_cache[text_hash].copy()

    model = load_embedding_model()
    embedding = model.encode(
        [text],
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False,
        batch_size=1,  # ë°°ì¹˜ í¬ê¸° ëª…ì‹œ
    )[0].astype("float32")

    if embedding.shape[0] != EMBEDDING_DIMENSION:
        raise ValueError(
            f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ëŒ€ê°’={EMBEDDING_DIMENSION}, ì‹¤ì œê°’={embedding.shape[0]}"
        )

    # ìºì‹œ ì €ì¥ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
    if use_cache:
        with _embedding_cache_lock:
            # ìºì‹œ í¬ê¸° ì œí•œ (LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ ê²ƒ ì œê±°)
            if len(_embedding_cache) >= EMBEDDING_CACHE_SIZE:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (ê°„ë‹¨í•œ ë°©ì‹: ëœë¤ ì œê±°)
                oldest_key = next(iter(_embedding_cache))
                del _embedding_cache[oldest_key]
            _embedding_cache[text_hash] = embedding.copy()

    return embedding


def create_embedding_vectors_batch(texts: list[str], use_cache: bool = True):
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì )
    
    ë°°ì¹˜ ì²˜ë¦¬ë¡œ GPU í™œìš©ë„ í–¥ìƒ ë° ì²˜ë¦¬ ì†ë„ ê°œì„ 
    
    Args:
        texts (list[str]): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        use_cache (bool): ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
    Returns:
        list[numpy.ndarray]: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    if not texts:
        return []
    
    # ìºì‹œ í™•ì¸ ë° ë¯¸ìºì‹œëœ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
    uncached_texts = []
    uncached_indices = []
    cached_embeddings = {}
    
    if use_cache:
        for idx, text in enumerate(texts):
            if not text or not text.strip():
                cached_embeddings[idx] = None
                continue
            text_hash = _get_text_hash(text)
            with _embedding_cache_lock:
                if text_hash in _embedding_cache:
                    cached_embeddings[idx] = _embedding_cache[text_hash].copy()
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(idx)
    else:
        uncached_texts = [t for t in texts if t and t.strip()]
        uncached_indices = list(range(len(uncached_texts)))
    
    # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
    if uncached_texts:
        model = load_embedding_model()
        embeddings = model.encode(
            uncached_texts,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False,
            batch_size=32,  # ë°°ì¹˜ í¬ê¸° ìµœì í™”
        ).astype("float32")
        
        # ìºì‹œ ì €ì¥
        if use_cache:
            with _embedding_cache_lock:
                for text, embedding in zip(uncached_texts, embeddings):
                    text_hash = _get_text_hash(text)
                    if len(_embedding_cache) >= EMBEDDING_CACHE_SIZE:
                        oldest_key = next(iter(_embedding_cache))
                        del _embedding_cache[oldest_key]
                    _embedding_cache[text_hash] = embedding.copy()
        
        # ê²°ê³¼ ë§¤í•‘
        for idx, embedding in zip(uncached_indices, embeddings):
            cached_embeddings[idx] = embedding
    
    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ë°˜í™˜
    result = []
    for i in range(len(texts)):
        if i in cached_embeddings:
            result.append(cached_embeddings[i])
        else:
            result.append(None)
    
    return result

def create_embedding_from_image(image_bytes):
    """
    ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ê¸°ë°˜ ê²€ìƒ‰ìš©)
    
    TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
    
    Args:
        image_bytes (bytes): ê²€ìƒ‰ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ íŒŒì¼ì˜ ë°”ì´íŠ¸ ë°ì´í„°
        
    Returns:
        numpy.ndarray: shape (EMBEDDING_DIMENSION,) ì„ë² ë”© ë²¡í„°
        
    êµ¬í˜„ ê°€ì´ë“œ:
        1. CLIP ë˜ëŠ” ìœ ì‚¬í•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì‚¬ìš©
        2. ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        3. í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ê°™ì€ ê³µê°„ì— ë§¤í•‘ë˜ë„ë¡ ì²˜ë¦¬
        4. ì •ê·œí™” ì ìš©
    """
    caption = describe_image_with_llava(image_bytes)
    if not caption:
        raise ValueError("ì´ë¯¸ì§€ ìº¡ì…”ë‹ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return create_embedding_vector(caption)


def warmup_models():
    """ì„œë²„ ê¸°ë™ ì‹œ ì£¼ìš” ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì½œë“œìŠ¤íƒ€íŠ¸ë¥¼ ì¤„ì„."""
    global models_warmed
    if models_warmed:
        return
    try:
        print("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
        load_embedding_model()
        preprocess_text("ëª¨ë¸ ì›Œë°ì—…")
        models_warmed = True
        print("âœ… ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
    except Exception as exc:
        print(f"âš ï¸ ëª¨ë¸ ì›Œë°ì—… ì‹¤íŒ¨: {exc}")


models_warmed = False
# ê° ì›Œì»¤ ì‹œì‘ ì‹œ ëª¨ë¸ê³¼ FAISS ë¯¸ë¦¬ ë¡œë“œ
initialize_faiss()
warmup_models()


@app.route('/health')
def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'service': 'lostfound-ai-server',
        'faiss_vectors': faiss_index.ntotal if faiss_index else 0
    })

@app.route('/api/v1/embedding/create', methods=['POST'])
def create_embedding():
    """
    ë¶„ì‹¤ë¬¼ ë“±ë¡ ì‹œ ì„ë² ë”© ìƒì„± ë° FAISS ì €ì¥
    
    í”„ë¡œì„¸ìŠ¤:
    1. ì‚¬ìš©ìê°€ ë“±ë¡í•œ ì´ë¯¸ì§€ë¥¼ LLaVAë¡œ ë¶„ì„í•˜ì—¬ ìì—°ì–´ ì„¤ëª… ìƒì„±
    2. (ì´ë¯¸ì§€ ì„¤ëª… + ì‚¬ìš©ì ì…ë ¥ ì„¤ëª…)ì„ BGE-M3ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    3. ì„ë² ë”© ë²¡í„°ë¥¼ FAISS ì¸ë±ìŠ¤ì— ì €ì¥
    4. MySQL item_idì™€ FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ë§¤í•‘í•˜ì—¬ ì €ì¥
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - item_id: MySQL ë¶„ì‹¤ë¬¼ ID (í•„ìˆ˜)
    - description: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¶„ì‹¤ë¬¼ ì„¤ëª… (ì„ íƒ)
    - image: ë¶„ì‹¤ë¬¼ ì´ë¯¸ì§€ íŒŒì¼ (ì„ íƒ)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_id: ì›ë³¸ ID (í™•ì¸ìš©)
    - message: ê²°ê³¼ ë©”ì‹œì§€
    """
    try:
        item_id = request.form.get('item_id')
        raw_description = request.form.get('description', '')
        image_file = request.files.get('image')
        
        if not item_id:
            return jsonify({'success': False, 'message': 'item_id í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        # 1. ì´ë¯¸ì§€ ë¬˜ì‚¬ ìƒì„± (Qwen ê¸°ë°˜)
        image_description = ""
        if image_file:
            image_bytes = image_file.read()
            image_description = describe_image_with_llava(image_bytes)
            if not image_description and raw_description:
                image_description = raw_description.strip()
            print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ (ì›ë³¸): {image_description[:100]}...")
        
        # 2. ì´ë¯¸ì§€ ë¬˜ì‚¬ + ì‚¬ìš©ì ì„¤ëª… ê²°í•© (ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ê²°í•©)
        #    ì˜ˆ) "ë¹¨ê°„ìƒ‰ ê°€ì£½ ì§€ê°‘ì…ë‹ˆë‹¤. ì‹ ì´Œì—­ 3ë²ˆ ì¶œêµ¬ì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤."
        parts = []
        if image_description:
            parts.append(image_description)
        if raw_description and raw_description.strip():
            parts.append(raw_description.strip())
        
        if not parts:
            return jsonify({'success': False, 'message': 'ì„¤ëª… ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ê²°í•©
        raw_full_text = " ".join(parts).strip()
        
        # 3. í†µí•© ì „ì²˜ë¦¬ (ê²€ìƒ‰ ì‹œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©)
        #    ì €ì¥ ì‹œì™€ ê²€ìƒ‰ ì‹œ ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì—¬ ì¼ê´€ì„± ë³´ì¥
        #    ë¦¬ì†ŒìŠ¤ ì ˆì•½: ë“±ë¡ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì •ì„ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
        final_text = preprocess_text(
            raw_full_text, 
            use_typo_correction=True,  # ë“±ë¡ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ì‚¬ìš©
            optimize_for_search=True   # ê²€ìƒ‰ ìµœì í™” ì ìš©
        )
        if not final_text or len(final_text.strip()) == 0:
            # ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš© (ê³µë°± ì œê±°ë§Œ)
            final_text = raw_full_text.strip()
        
        print(f"ğŸ§¾ ì„ë² ë”© í…ìŠ¤íŠ¸: item_id={item_id}")
        print(f"   ì›ë³¸: {raw_full_text[:100]}...")
        print(f"   ì „ì²˜ë¦¬ í›„: {final_text[:100]}...")
        
        # 4. í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (BGE-M3 ì‚¬ìš©)
        #    ê²€ìƒ‰ ì‹œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
        embedding_vector = create_embedding_vector(final_text)
        
        # 4. FAISS ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€ (ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        with _faiss_lock:
            faiss_index.add(np.array([embedding_vector]))
            faiss_idx = faiss_index.ntotal - 1
            # 5. FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ â†” MySQL item_id ë§¤í•‘ ì €ì¥
            id_mapping[faiss_idx] = int(item_id)
        
        # 6. FAISS ì¸ë±ìŠ¤ ë° ë§¤í•‘ ì •ë³´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ì˜ì†ì„±)
        save_faiss()
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: item_id={item_id}, faiss_idx={faiss_idx}, ë²¡í„° ì°¨ì›={len(embedding_vector)}")
        
        return jsonify({
            'success': True,
            'item_id': int(item_id),
            'message': f'ì„ë² ë”© ìƒì„± ì™„ë£Œ (FAISS ì¸ë±ìŠ¤: {faiss_idx})'
        })
        
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/create-batch', methods=['POST'])
def create_embeddings_batch():
    """
    ë¶„ì‹¤ë¬¼ ë“±ë¡ ì‹œ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ë° FAISS ì €ì¥ (ì„±ëŠ¥ ìµœì í™”)
    
    ì—¬ëŸ¬ ì•„ì´í…œì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ ë° ì²˜ë¦¬ ì†ë„ í–¥ìƒ
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - items: ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ê° ì•„ì´í…œì€ ë‹¤ìŒ í•„ë“œ í¬í•¨)
      - item_id: MySQL ë¶„ì‹¤ë¬¼ ID (í•„ìˆ˜)
      - description: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¶„ì‹¤ë¬¼ ì„¤ëª… (ì„ íƒ)
      - image_url: ì´ë¯¸ì§€ URL (ì„ íƒ)
      - image: ì´ë¯¸ì§€ íŒŒì¼ (image_urlì´ ì—†ì„ ê²½ìš°, ì„ íƒ)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - results: ê° ì•„ì´í…œë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
      - item_id: ì›ë³¸ ID
      - success: ì„±ê³µ ì—¬ë¶€
      - message: ê²°ê³¼ ë©”ì‹œì§€
      - faiss_idx: FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ (ì„±ê³µ ì‹œ)
    """
    try:
        items_data = request.form.get('items')
        if not items_data:
            return jsonify({'success': False, 'message': 'items ë°ì´í„° í•„ìš”'}), 400
        
        try:
            items = json.loads(items_data)
        except json.JSONDecodeError:
            return jsonify({'success': False, 'message': 'items JSON íŒŒì‹± ì‹¤íŒ¨'}), 400
        
        if not isinstance(items, list) or len(items) == 0:
            return jsonify({'success': False, 'message': 'itemsëŠ” ë¹„ì–´ìˆì§€ ì•Šì€ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸
        if not _faiss_initialized:
            initialize_faiss()
        
        results = []
        successful_count = 0
        
        # ë°°ì¹˜ ì²˜ë¦¬: ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        def process_item(item):
            """ë‹¨ì¼ ì•„ì´í…œ ì²˜ë¦¬"""
            item_id = item.get('item_id')
            raw_description = item.get('description', '')
            image_url = item.get('image_url', '')
            
            if not item_id:
                return {
                    'item_id': None,
                    'success': False,
                    'message': 'item_id í•„ìš”'
                }
            
            try:
                # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ìº¡ì…”ë‹ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                image_description = ""
                if image_url:
                    max_retries = 2
                    for attempt in range(max_retries):
                        try:
                            response = requests.get(
                                image_url, 
                                timeout=(5, 15),  # (ì—°ê²° íƒ€ì„ì•„ì›ƒ, ì½ê¸° íƒ€ì„ì•„ì›ƒ)
                                stream=True,
                                headers={'User-Agent': 'Mozilla/5.0'}  # ì¼ë¶€ ì„œë²„ì—ì„œ í•„ìš”
                            )
                            response.raise_for_status()
                            image_bytes = response.content
                            if len(image_bytes) > 20 * 1024 * 1024:  # 20MB ì œí•œ
                                raise ValueError("ì´ë¯¸ì§€ íŒŒì¼ì´ ë„ˆë¬´ í¼")
                            if len(image_bytes) == 0:
                                raise ValueError("ì´ë¯¸ì§€ íŒŒì¼ì´ ë¹„ì–´ìˆìŒ")
                            image_description = describe_image_with_llava(image_bytes)
                            break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
                        except Exception as e:
                            if attempt == max_retries - 1:
                                print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ/ìº¡ì…”ë‹ ì‹¤íŒ¨ (item_id={item_id}, ì‹œë„ {attempt+1}/{max_retries}): {e}")
                            else:
                                time.sleep(0.5 * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                            # ë§ˆì§€ë§‰ ì‹œë„ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ì§„í–‰
                
                # 2. í…ìŠ¤íŠ¸ ê²°í•© ë° ì „ì²˜ë¦¬
                parts = []
                if image_description:
                    parts.append(image_description)
                if raw_description and raw_description.strip():
                    parts.append(raw_description.strip())
                
                if not parts:
                    return {
                        'item_id': int(item_id),
                        'success': False,
                        'message': 'ì„¤ëª… ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤'
                    }
                
                raw_full_text = " ".join(parts).strip()
                final_text = preprocess_text(
                    raw_full_text,
                    use_typo_correction=True,
                    optimize_for_search=True
                )
                if not final_text or len(final_text.strip()) == 0:
                    final_text = raw_full_text.strip()
                
                # 3. ì„ë² ë”© ë²¡í„° ìƒì„± (ìºì‹œ í™œìš©)
                embedding_vector = create_embedding_vector(final_text, use_cache=True)
                
                # 4. FAISS ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€ (ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                faiss_idx = None
                with _faiss_lock:
                    faiss_index.add(np.array([embedding_vector]))
                    faiss_idx = faiss_index.ntotal - 1
                    id_mapping[faiss_idx] = int(item_id)
                
                return {
                    'item_id': int(item_id),
                    'success': True,
                    'message': f'ì„ë² ë”© ìƒì„± ì™„ë£Œ',
                    'faiss_idx': faiss_idx
                }
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (item_id={item_id}): {str(e)}")
                return {
                    'item_id': int(item_id) if item_id else None,
                    'success': False,
                    'message': str(e)
                }
        
        # ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 10ê°œ ë™ì‹œ ì²˜ë¦¬)
        max_workers = min(10, len(items))
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_item = {executor.submit(process_item, item): item for item in items}
            for future in concurrent.futures.as_completed(future_to_item):
                result = future.result()
                results.append(result)
                if result.get('success'):
                    successful_count += 1
        
        # FAISS ì¸ë±ìŠ¤ ë° ë§¤í•‘ ì •ë³´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ë°°ì¹˜ ì™„ë£Œ í›„ í•œ ë²ˆë§Œ)
        save_faiss()
        
        print(f"âœ… ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {successful_count}/{len(items)}ê°œ ì„±ê³µ")
        
        return jsonify({
            'success': True,
            'results': results,
            'summary': {
                'total': len(items),
                'successful': successful_count,
                'failed': len(items) - successful_count
            }
        })
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/search', methods=['POST'])
def search_embedding():
    """
    ìì—°ì–´ ê²€ìƒ‰: í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ê²€ìƒ‰
    
    í”„ë¡œì„¸ìŠ¤:
    1. ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ BGE-M3ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    2. FAISSì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ Top-K ê²€ìƒ‰
    3. ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ MySQL item_id ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - query: ìì—°ì–´ ê²€ìƒ‰ì–´ (í•„ìˆ˜)
      ì˜ˆ) "ì§€í•˜ì² ì—ì„œ ìƒì–´ë²„ë¦° ê²€ì€ ì§€ê°‘", "ê°•ë‚¨ì—­ì—ì„œ ë°œê²¬í•œ ì•„ì´í°"
    - top_k: ë°˜í™˜í•  ê°œìˆ˜ (ì„ íƒ, ê¸°ë³¸ 10)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_ids: ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ MySQL item_id ë¦¬ìŠ¤íŠ¸
    
    TODO: AI íŒ€ ì¶”ê°€ êµ¬í˜„ ì‚¬í•­
    - ë‚ ì§œ/ì¥ì†Œ í•„í„°ë§ ê°€ì¤‘ì¹˜ ì ìš©
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì‹œë§¨í‹±)
    """
    try:
        data = request.get_json()
        raw_query = data.get('query', '')
        if not raw_query or not raw_query.strip():
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ë¦¬ì†ŒìŠ¤ ì ˆì•½: ê²€ìƒ‰ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ì„ íƒì )
        query = preprocess_text(
            raw_query,
            use_typo_correction=False,  # ê²€ìƒ‰ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ìŠ¤í‚µí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½
            optimize_for_search=True    # ê²€ìƒ‰ ìµœì í™” ì ìš©
        )
        if not query:
            query = raw_query.strip()
        
        top_k = data.get('top_k', 10)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´ ì¶”ê°€ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
        # ë¦¬ì†ŒìŠ¤ ì ˆì•½: ì‚¬ì „ ê¸°ë°˜ í™•ì¥ìœ¼ë¡œ LLM ì—†ì´ ë¹ ë¥´ê²Œ ì²˜ë¦¬
        expanded_queries = expand_search_query(query)
        if len(expanded_queries) > 1:
            print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥: ì›ë³¸='{query}', í™•ì¥={expanded_queries[:3]}")
        
        if not query:
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        # FAISS ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if faiss_index is None or faiss_index.ntotal == 0:
            return jsonify({'success': True, 'item_ids': []})
        
        # 1. ê²€ìƒ‰ì–´ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (BGE-M3 ì‚¬ìš©, ìºì‹œ í™œìš©)
        #    í™•ì¥ëœ ì¿¼ë¦¬ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ í–¥ìƒ
        if len(expanded_queries) > 1:
            # ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© (ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì )
            query_vectors = create_embedding_vectors_batch(expanded_queries, use_cache=True)
            # ì›ë³¸ ì¿¼ë¦¬ ë²¡í„°ë¥¼ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©
            query_vector = query_vectors[0]
        else:
            query_vector = create_embedding_vector(query, use_cache=True)
        
        # 2. FAISSì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ Top-K ê²€ìƒ‰
        k = min(top_k * 2, faiss_index.ntotal)  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ ì¬ì •ë ¬ ì—¬ìœ  í™•ë³´
        
        # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search íŒŒë¼ë¯¸í„° ì„¤ì • (ì •í™•ë„ì™€ ì„±ëŠ¥ ê· í˜•)
        if hasattr(faiss_index, 'hnsw'):
            # kë³´ë‹¤ ì¶©ë¶„íˆ í° ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
            faiss_index.hnsw.efSearch = max(HNSW_EF_SEARCH, k * 2)  # 3 -> 2ë¡œ ì¤„ì—¬ì„œ ë¦¬ì†ŒìŠ¤ ì ˆì•½
        
        # ê²€ìƒ‰ ì‹¤í–‰
        distances, indices = faiss_index.search(np.array([query_vector]), k)
        
        # í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ê²€ìƒ‰ (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
        if len(expanded_queries) > 1 and len(query_vectors) > 1:
            all_candidates = {}  # item_id -> ìµœê³  ì ìˆ˜
            for qv in query_vectors[1:]:  # ì›ë³¸ ì œì™¸í•œ í™•ì¥ ì¿¼ë¦¬ë“¤
                dists, idxs = faiss_index.search(np.array([qv]), k)
                for idx, dist in zip(idxs[0], dists[0]):
                    if int(idx) != -1 and int(idx) in id_mapping:
                        item_id = id_mapping[int(idx)]
                        # ì—¬ëŸ¬ ì¿¼ë¦¬ ì¤‘ ìµœê³  ì ìˆ˜ ìœ ì§€
                        if item_id not in all_candidates or dist > all_candidates[item_id]:
                            all_candidates[item_id] = dist
            
            # ì›ë³¸ ì¿¼ë¦¬ ê²°ê³¼ì™€ ë³‘í•©
            for idx, dist in zip(indices[0], distances[0]):
                if int(idx) != -1 and int(idx) in id_mapping:
                    item_id = id_mapping[int(idx)]
                    if item_id not in all_candidates or dist > all_candidates[item_id]:
                        all_candidates[item_id] = dist
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ ë° ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            sorted_candidates = sorted(all_candidates.items(), key=lambda x: x[1], reverse=True)
            # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í•„í„°ë§
            filtered_candidates = [
                (item_id, score) for item_id, score in sorted_candidates
                if score >= SIMILARITY_THRESHOLD
            ]
            
            # ìµœì†Œ ê²°ê³¼ ìˆ˜ ë³´ì¥ (ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì ì–´ë„ ìµœì†Œ ê°œìˆ˜ëŠ” ë°˜í™˜)
            if len(filtered_candidates) < MIN_RESULTS_TO_RETURN and len(sorted_candidates) > 0:
                # ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì ìœ¼ë©´ ìƒìœ„ ê²°ê³¼ë¥¼ í¬í•¨ (ìµœì†Œ ê°œìˆ˜ ë³´ì¥)
                filtered_candidates = sorted_candidates[:max(MIN_RESULTS_TO_RETURN, top_k)]
            
            item_ids = [item_id for item_id, _ in filtered_candidates[:top_k]]
            scores = [score for _, score in filtered_candidates[:top_k]]
        else:
            # ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰
            debug_pairs = [
                (int(idx), float(dist))
                for idx, dist in zip(indices[0], distances[0])
                if idx != -1
            ]
            print(f"ğŸ“ˆ ê²€ìƒ‰ ë””ë²„ê·¸: query='{query[:50]}', ê²°ê³¼={debug_pairs}")
            
            # FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ â†’ MySQL item_id ë³€í™˜ ë° ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            item_ids = []
            scores = []
            for idx, dist in zip(indices[0], distances[0]):
                if int(idx) != -1 and int(idx) in id_mapping:
                    score = float(dist)  # IndexFlatIPì´ë¯€ë¡œ ë‚´ì  ê°’ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                    # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í¬í•¨
                    if score >= SIMILARITY_THRESHOLD:
                        item_ids.append(id_mapping[int(idx)])
                        scores.append(score)
            
            # ìµœì†Œ ê²°ê³¼ ìˆ˜ ë³´ì¥ (ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì ì–´ë„ ìµœì†Œ ê°œìˆ˜ëŠ” ë°˜í™˜)
            if len(item_ids) < MIN_RESULTS_TO_RETURN:
                # ì„ê³„ê°’ ë¯¸ë§Œì´ì–´ë„ ìƒìœ„ ê²°ê³¼ë¥¼ í¬í•¨ (ìµœì†Œ ê°œìˆ˜ ë³´ì¥)
                item_ids = []
                scores = []
                for idx, dist in zip(indices[0], distances[0]):
                    if int(idx) != -1 and int(idx) in id_mapping:
                        item_ids.append(id_mapping[int(idx)])
                        scores.append(float(dist))
                        if len(item_ids) >= MIN_RESULTS_TO_RETURN:
                            break
            
            # top_kë§Œí¼ë§Œ ë°˜í™˜
            item_ids = item_ids[:top_k]
            scores = scores[:top_k]
        
        # ë””ë²„ê¹…: ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ì¶œë ¥
        result_pairs = list(zip(item_ids[:10], scores[:10]))
        filtered_count = len([s for s in scores if s >= SIMILARITY_THRESHOLD])
        print(f"ğŸ” ìì—°ì–´ ê²€ìƒ‰ ì™„ë£Œ: query='{query[:30]}...', top_k={top_k}, ê²°ê³¼={len(item_ids)}ê°œ")
        print(f"ğŸ“Š ìœ ì‚¬ë„ ì„ê³„ê°’: {SIMILARITY_THRESHOLD}, ì„ê³„ê°’ ì´ìƒ: {filtered_count}ê°œ")
        print(f"ğŸ“Š ìƒìœ„ 10ê°œ ìœ ì‚¬ë„ ì ìˆ˜: {result_pairs}")
        
        return jsonify({
            'success': True,
            'item_ids': item_ids,
            'scores': scores  # ìœ ì‚¬ë„ ì ìˆ˜ë„ í•¨ê»˜ ë°˜í™˜
        })
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/search-by-image', methods=['POST'])
def search_by_image():
    """
    ì´ë¯¸ì§€ ê¸°ë°˜ ê²€ìƒ‰: ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ê²€ìƒ‰
    
    í”„ë¡œì„¸ìŠ¤:
    1. ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ LLaVA ë˜ëŠ” CLIPìœ¼ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    2. FAISSì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
    3. ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œëŒ€ë¡œ MySQL item_id ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - image: ê²€ìƒ‰ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ íŒŒì¼ (í•„ìˆ˜)
    - top_k: ë°˜í™˜í•  ê°œìˆ˜ (ì„ íƒ, ê¸°ë³¸ 10)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_ids: ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ MySQL item_id ë¦¬ìŠ¤íŠ¸
    
    TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
    - create_embedding_from_image() í•¨ìˆ˜ êµ¬í˜„
    - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (CLIP ë“±) ì‚¬ìš©
    """
    try:
        image_file = request.files.get('image')
        top_k = int(request.form.get('top_k', 10))
        
        if not image_file:
            return jsonify({'success': False, 'message': 'ì´ë¯¸ì§€ íŒŒì¼ í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        if faiss_index is None or faiss_index.ntotal == 0:
            return jsonify({'success': True, 'item_ids': []})
        
        # 1. ì´ë¯¸ì§€ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        #    AI íŒ€: create_embedding_from_image() í•¨ìˆ˜ êµ¬í˜„ í•„ìš”
        image_bytes = image_file.read()
        try:
            query_vector = create_embedding_from_image(image_bytes)
        except ValueError as err:
            return jsonify({'success': False, 'message': str(err)}), 400
        
        # 2. FAISSì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
        k = min(top_k, faiss_index.ntotal)
        
        # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search íŒŒë¼ë¯¸í„° ì„¤ì •
        if hasattr(faiss_index, 'hnsw'):
            faiss_index.hnsw.efSearch = max(HNSW_EF_SEARCH, k * 2)
        
        distances, indices = faiss_index.search(np.array([query_vector]), k)
        
        # 3. FAISS ì¸ë±ìŠ¤ ë²ˆí˜¸ â†’ MySQL item_id ë³€í™˜ ë° ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
        item_ids = []
        scores = []
        for idx, dist in zip(indices[0], distances[0]):
            if int(idx) != -1 and int(idx) in id_mapping:
                score = float(dist)
                # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²°ê³¼ë§Œ í¬í•¨
                if score >= SIMILARITY_THRESHOLD:
                    item_ids.append(id_mapping[int(idx)])
                    scores.append(score)
        
        # ìµœì†Œ ê²°ê³¼ ìˆ˜ ë³´ì¥
        if len(item_ids) < MIN_RESULTS_TO_RETURN:
            item_ids = []
            scores = []
            for idx, dist in zip(indices[0], distances[0]):
                if int(idx) != -1 and int(idx) in id_mapping:
                    item_ids.append(id_mapping[int(idx)])
                    scores.append(float(dist))
                    if len(item_ids) >= MIN_RESULTS_TO_RETURN:
                        break
        
        # top_kë§Œí¼ë§Œ ë°˜í™˜
        item_ids = item_ids[:top_k]
        scores = scores[:top_k] if scores else []
        
        filtered_count = len([s for s in scores if s >= SIMILARITY_THRESHOLD]) if scores else len(item_ids)
        print(f"ğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰ ì™„ë£Œ: top_k={top_k}, ê²°ê³¼={len(item_ids)}ê°œ, ì„ê³„ê°’ ì´ìƒ: {filtered_count}ê°œ")
        
        return jsonify({
            'success': True,
            'item_ids': item_ids,
            'scores': scores  # ìœ ì‚¬ë„ ì ìˆ˜ë„ í•¨ê»˜ ë°˜í™˜
        })
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/search-with-filters', methods=['POST'])
def search_with_filters():
    """
    í•„í„°ë§ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ê³ ê¸‰ ê²€ìƒ‰
    
    í”„ë¡œì„¸ìŠ¤:
    1. ìì—°ì–´ ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ í›„ë³´ ì¶”ì¶œ
    2. ë‚ ì§œ, ì¥ì†Œ í•„í„°ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì ìš©
    3. ì¬ì •ë ¬í•˜ì—¬ ê²°ê³¼ ë°˜í™˜
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - query: ìì—°ì–´ ê²€ìƒ‰ì–´ (í•„ìˆ˜)
    - top_k: ë°˜í™˜í•  ê°œìˆ˜ (ì„ íƒ, ê¸°ë³¸ 10)
    - filters: í•„í„°ë§ ì¡°ê±´ (ì„ íƒ)
      - location: ì¥ì†Œ (ì˜ˆ: "ê°•ë‚¨ì—­")
      - start_date: ì‹œì‘ ë‚ ì§œ (ì˜ˆ: "2025-01-01")
      - end_date: ì¢…ë£Œ ë‚ ì§œ (ì˜ˆ: "2025-01-31")
    - weights: ê°€ì¤‘ì¹˜ ì„¤ì • (ì„ íƒ)
      - semantic: ì‹œë§¨í‹± ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ 0.7)
      - location: ì¥ì†Œ ì¼ì¹˜ ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ 0.2)
      - date: ë‚ ì§œ ì¼ì¹˜ ê°€ì¤‘ì¹˜ (0~1, ê¸°ë³¸ 0.1)
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    - item_ids: ì¬ì •ë ¬ëœ MySQL item_id ë¦¬ìŠ¤íŠ¸
    
    TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
    - í•„í„°ë§ ë¡œì§ êµ¬í˜„
    - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ
    - Springì—ì„œ ë©”íƒ€ë°ì´í„° í•¨ê»˜ ì „ë‹¬ë°›ëŠ” ë°©ì‹ ê³ ë ¤
    
    ì°¸ê³ :
    í˜„ì¬ëŠ” ê¸°ë³¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•˜ë©°, í•„í„°ë§ì€ Spring ë‹¨ì—ì„œ ì²˜ë¦¬ë¨
    í–¥í›„ AI ë‹¨ì—ì„œ í•„í„°ë§ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ê³ ê¸‰ ê²€ìƒ‰ êµ¬í˜„ ê°€ëŠ¥
    """
    try:
        data = request.get_json()
        raw_query = data.get('query', '')
        if not raw_query or not raw_query.strip():
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
        query = preprocess_text(
            raw_query,
            use_typo_correction=False,  # ê²€ìƒ‰ ì‹œì—ëŠ” ë§ì¶¤ë²• êµì • ìŠ¤í‚µ
            optimize_for_search=True
        )
        if not query:
            query = raw_query.strip()
        
        top_k = data.get('top_k', 10)
        filters = data.get('filters', {})
        weights = data.get('weights', {
            'semantic': 0.7,
            'location': 0.2,
            'date': 0.1
        })
        
        if not query:
            return jsonify({'success': False, 'message': 'ê²€ìƒ‰ì–´ í•„ìš”'}), 400
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” í™•ì¸ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if not _faiss_initialized:
            initialize_faiss()
        
        if faiss_index is None or faiss_index.ntotal == 0:
            return jsonify({'success': True, 'item_ids': []})
        
        # 1. ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰ (ìºì‹œ í™œìš©)
        query_vector = create_embedding_vector(query, use_cache=True)
        k = min(top_k * 3, faiss_index.ntotal)  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        
        # HNSW ì¸ë±ìŠ¤ì¸ ê²½ìš° ef_search íŒŒë¼ë¯¸í„° ì„¤ì •
        if hasattr(faiss_index, 'hnsw'):
            faiss_index.hnsw.efSearch = max(HNSW_EF_SEARCH, k * 2)
        
        distances, indices = faiss_index.search(np.array([query_vector]), k)
        
        # 2. ì´ˆê¸° í›„ë³´ ì¶”ì¶œ
        item_ids = []
        for idx in indices[0]:
            if int(idx) in id_mapping:
                item_ids.append(id_mapping[int(idx)])
        
        # TODO: AI íŒ€ êµ¬í˜„ í•„ìš”
        # 3. í•„í„°ë§ ê°€ì¤‘ì¹˜ ì ìš© ë° ì¬ì •ë ¬
        # - Springì—ì„œ ê° itemì˜ ë©”íƒ€ë°ì´í„°(ì¥ì†Œ, ë‚ ì§œ ë“±)ë¥¼ ë°›ì•„ì•¼ í•¨
        # - ë˜ëŠ” Flaskì—ì„œ ë³„ë„ DB ì—°ê²°í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        # - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìŠ¤ì½”ì–´ ê³„ì‚°: score = w1*sim + w2*loc_match + w3*date_match
        # - ìŠ¤ì½”ì–´ ê¸°ë°˜ ì¬ì •ë ¬
        
        print(f"ğŸ” í•„í„°ë§ ê²€ìƒ‰ ì™„ë£Œ: query='{query[:30]}...', ê²°ê³¼={len(item_ids[:top_k])}ê°œ")
        
        return jsonify({
            'success': True,
            'item_ids': item_ids[:top_k]
        })
        
    except Exception as e:
        print(f"âŒ í•„í„°ë§ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

@app.route('/api/v1/embedding/delete/<int:item_id>', methods=['DELETE'])
def delete_embedding(item_id):
    """
    ë¶„ì‹¤ë¬¼ ì‚­ì œ ì‹œ ì„ë² ë”© ì œê±°
    
    í”„ë¡œì„¸ìŠ¤:
    - FAISSì—ì„œ ë¬¼ë¦¬ì  ì‚­ì œëŠ” í•˜ì§€ ì•ŠìŒ (ì„±ëŠ¥ ì´ìŠˆ)
    - id_mappingì—ì„œë§Œ ì œê±°í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ì— ë‚˜íƒ€ë‚˜ì§€ ì•Šë„ë¡ í•¨
    
    Springì—ì„œ ë°›ëŠ” ê²ƒ:
    - item_id: ì‚­ì œí•  ë¶„ì‹¤ë¬¼ì˜ MySQL ID
    
    Springìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒ:
    - success: ì„±ê³µ ì—¬ë¶€
    """
    try:
        # ë§¤í•‘ì—ì„œ ì œê±°
        deleted = [k for k, v in id_mapping.items() if v == item_id]
        for k in deleted:
            del id_mapping[k]
        
        save_faiss()
        print(f"ğŸ—‘ï¸  ì‚­ì œ: item_id={item_id}, ì œê±°ëœ ë²¡í„°={len(deleted)}ê°œ")
        
        return jsonify({'success': True})
        
    except Exception as e:
        print(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return jsonify({'success': False, 'message': str(e)}), 500

# Gunicornìœ¼ë¡œ ì‹¤í–‰í•  ë•Œë„ FAISS ì´ˆê¸°í™”ëŠ” warmup_models()ì—ì„œ ìˆ˜í–‰ë¨
# ê° ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ê°€ ì‹œì‘ë  ë•Œë§ˆë‹¤ ì´ˆê¸°í™”ë¨

if __name__ == '__main__':
    # ê°œë°œ ëª¨ë“œì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ì‚¬ìš©
    port = int(os.environ.get('PORT', 5001))
    debug = os.environ.get('DEBUG', 'False').lower() == 'true'
    
    app.run(
        host='0.0.0.0',
        port=port,
        debug=debug
    )
